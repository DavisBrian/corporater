[
["index.html", "Modern R in a Corporate Environment Welcome", " Modern R in a Corporate Environment R course developed for the office Brian Davis 2018-07-02 Welcome Something that will make life easier in the long-run can be the most difficult thing to do today. For coders, prioritising the long term may involve an overhaul of current practice and the learning of a new skill. This is the course notes for our class. This course will teach you how to do data science with R. You’ll learn the basics of R and then we’ll go through R for Data Science by Garrett Grolemund &amp; Hadley Wickham. You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualize it and communicate out your results. We’ll mix in various topics from our current workload as well as some unique challenges of working in a corporate environment. Most of these are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time and reduce errors. We will build the tools to make our work easier and more streamlined together. "],
["preamble-intro.html", "1 Introduction 1.1 Course Philosophy 1.2 Prerequisites 1.3 Content 1.4 Structure", " 1 Introduction 1.1 Course Philosophy “The best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct.” — Donald Knuth 1.1.1 Reproducible Research Approach What is Reproducible Research About? Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. There are two basic reasons to be concerned about making your research reproducible. The first is to show evidence of the correctness of your results. The second reason to aspire to reproducibility is to enable others to make use of our methods and results. Modern challenges of reproducibility in research, particularly computational reproducibility, have produced a lot of discussion in papers, blogs and videos, some of which are listed here and here. Conclusions in experimental psychology often are the result of null hypothesis significance testing. Unfortunately, there is evidence ((from eight major psychology journals published between 1985 and 2013) that roughly half of all published empirical psychology articles contain at least one inconsistent p-value, and around one in eight articles contain a grossly inconsistent p-value that makes a non-significant result seem significant, or vice versa. statscheck and here “A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that were able to reproduce the findings for 26%.” Proceedings of the National Academy of Sciences of the United States of America “Starting September 1 2016, JASA ACS will require code and data as a minimum standard for reproducibility of statistical scientific research.” JASA 1.1.2 FDA Validation “Establishing documented evidence which provides a high degree of assurance that a specific process will consistently produce a product meeting its predetermined specifications and quality attributes.” -Validation as defined by the FDA in Validation of Systems for 21 CFR Part 11 Compliance 1.1.3 The SAS Myth Contrary to what we hear the FDA does not require SAS to be used EVER. There are instances that you have to deliver data in XPORT format though which is open and implemented in many programming languages. “FDA does not require use of any specific software for statistical analyses, and statistical software is not explicitly discussed in Title 21 of the Code of Federal Regulations [e.g., in 21CFR part 11]. However, the software package(s) used for statistical analyses should be fully documented in the submission, including version and build identification. As noted in the FDA guidance, E9 Statistical Principles for Clinical Trials” FDA Statistical Software Clarifying Statement Good write up with links to several FDA talks on the subject. 1.2 Prerequisites We will assume you have minimal experience and knowledge of R IT should have installed: R version 3.4.4 RStudio version 1.1 MiTeX RTools version 3.4 We will install other dependencies throughout the course. 1.3 Content It is impossible to become an expert in R in only one course even a multi-week one. Our aim is at gaining a wide understanding on many aspects of R as used in a corporate / production environment. It will roughly be based on R for Data Science. While this is an excellent resource it does not cover much of what we will need on a routine basis. Some external resources will be referred to in this book for you to be able to deepen what you would have learned in this course. This is your course so if you feel we need to hit an area deeper, or add content based on a current need, let me know an we will work to adjust it. The rough topic list of the course: Good programming practices Basics of R Programming Importing / Exporting Data Tidying Data Visualizing Data Functions Strings Dates and Time Communicating Results Making Code Production Ready: Functions (part II) Assertions Unit tests Documentation Communicating Results (part II) 1.4 Structure My current thoughts are to meet an hour a week and discuss a topic. We will not be going strictly through the R4DS, but will use it as our foundation into the topic at hand. Then give some exercises due for the next week which we go over the solutions. We will incorporate these exercises into an R package(s?) so we will have a collection of useful reusable code for the future. Open to other ideas as we go along. I’m going to try to keep the assignments related to our current work so we can work on the class during work hours. Bring what you are working on and we will see how we can fit it into the class. "],
["good-practices.html", "2 Good practices 2.1 Coding style 2.2 Coding practices 2.3 RStudio 2.4 Getting help 2.5 Keeping up to date 2.6 Exercises", " 2 Good practices “When you write a program, think of it primarily as a work of literature. You’re trying to write something that human beings are going to read. Don’t think of it primarily as something a computer is going to follow. The more effective you are at making your program readable, the more effective it’s going to be: You’ll understand it today, you’ll understand it next week, and your successors who are going to maintain and modify it will understand it.” – Donald Knuth 2.1 Coding style Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. When I answer questions; first, I see if think I can answer the question, secondly, I check the coding style of the question and if the code is too difficult to read, I just move on. Please make your code readable by following e.g. this coding style (most examples below come from this guide). “To become ssignificantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs.” — Bjarne Stroustrup 2.1.1 Comments In code, use comments to explain the “why” not the “what” or “how”. Each line of a comment should begin with the comment symbol and a single space: #. Use commented lines of - to break up your file into easily readable chunks and to create a code outline in RStudio 2.1.2 Naming There are only two hard things in Computer Science: cache invalidation and naming things. – Phil Karlton Names are not limited to 8 characters as in some other languages, however they are case sensitive. Be smart with your naming; be descriptive yet concise. Think about how your names will show up in auto complete. Throughout the course we will point out some standard naming conventions that are used in R (and other languages). (Ex. i and j as row and column indices) # Good average_height &lt;- mean((feet / 12) + inches) plot(mtcars$disp, mtcars$mpg) # Bad ah&lt;-mean(x/12+y) plot(mtcars[, 3], mtcars[, 1]) 2.1.3 Spacing Put a space before and after = when naming arguments in function calls. Most infix operators (==, +, -, &lt;-, etc.) are also surrounded by spaces, except those with relatively high precedence: ^, :, ::, and :::. Always put a space after a comma, and never before (just like in regular English). # Good average &lt;- mean((feet / 12) + inches, na.rm = TRUE) sqrt(x^2 + y^2) x &lt;- 1:10 base::sum # Bad average&lt;-mean(feet/12+inches,na.rm=TRUE) sqrt(x ^ 2 + y ^ 2) x &lt;- 1 : 10 base :: sum 2.1.4 Indenting Curly braces, {}, define the the most important hierarchy of R code. To make this hierarchy easy to see, always indent the code inside {} by two spaces. # Good if (y &lt; 0 &amp;&amp; debug) { message(&quot;y is negative&quot;) } if (y == 0) { if (x &gt; 0) { log(x) } else { message(&quot;x is negative or zero&quot;) } } else { y ^ x } # Bad if (y &lt; 0 &amp;&amp; debug) message(&quot;Y is negative&quot;) if (y == 0) { if (x &gt; 0) { log(x) } else { message(&quot;x is negative or zero&quot;) } } else { y ^ x } 2.1.5 Long lines Strive to limit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font. If you find yourself running out of room, this is a good indication that you should encapsulate some of the work into a separate function. If a function call is too long to fit on a single line, use one line each for the function name, each argument, and the closing ). This makes the code easier to read and to change later. # Good do_something_very_complicated( something = &quot;that&quot;, requires = many, arguments = &quot;some of which may be long&quot; ) # Bad do_something_very_complicated(&quot;that&quot;, requires, many, arguments, &quot;some of which may be long&quot; 2.1.6 Other Use &lt;-, not =, for assignment. Keep = for parameters. # Good x &lt;- 5 system.time( x &lt;- rnorm(1e6) ) # Bad x = 5 system.time( x = rnorm(1e6) ) Don’t put ; at the end of a line, and don’t use ; to put multiple commands on one line. Only use return() for early returns. Otherwise rely on R to return the result of the last evaluated expression. # Good add_two &lt;- function(x, y) { x + y } # Bad add_two &lt;- function(x, y) { return(x + y) } Use &quot;, not ', for quoting text. The only exception is when the text already contains double quotes and no single quotes. # Good &quot;Text&quot; &#39;Text with &quot;quotes&quot;&#39; &#39;&lt;a href=&quot;http://style.tidyverse.org&quot;&gt;A link&lt;/a&gt;&#39; # Bad &#39;Text&#39; &#39;Text with &quot;double&quot; and \\&#39;single\\&#39; quotes&#39; 2.2 Coding practices 2.2.1 Variables Create variables for values that are likely to change. 2.2.2 Rule of Three1 Try not to copy code, or copy then modify the code, more than twice. If a change requires you to search/replace 3 or more times make a variable. If you copy a code chunk 3 or more times make a function If you copy a function 3 or more times make your function more generic If you copy a function into a project 3 or more times make a package If 3 or more people will use the function make a package The Rule of Three applies to look-up tables and such also. The key thing to think about is; if something changes how many touch points will there be? If it is 3 or more places it is time to abstract this code a bit. 2.2.3 Path names It is better to use relative path names instead of hard coded ones. If you must read from (or write to) paths that are not in your project directory structure create a file name variable at the highest level you can (always end with the /) and then use relative paths. DO NOT EVER USE setwd() # Good raw_data &lt;- read.csv(&quot;./data/mydatafile.csv&quot;) input_file &lt;- &quot;./data/mydatafile.csv&quot; raw_data &lt;- read.csv(input_file) input_path &lt;- &quot;C:/Path/To/Some/other/project/directory/&quot; input_file &lt;- paste0(input_path, &quot;data/mydatafile.csv&quot;) raw_data &lt;- read.csv(input_file) # Bad setwd(&quot;C:/Path/To/Some/other/project/directory/data/&quot;) raw_data &lt;- read.csv(&quot;mydatafile.csv&quot;) setwd(&quot;C:/Path/back/to/my/project/&quot;) 2.3 RStudio Download the latest version of RStudio (&gt; 1.1) and use it! Learn more about new features of RStudio v1.1 there. RStudio features: everything you can expect from a good IDE keyboard shortcuts I use frequently Ctrl + Space (auto-completion, better than Tab) Ctrl + Up (command history &amp; search) Ctrl + Enter (execute line of code) Ctrl + Shift + A (reformat code) Ctrl + Shift + C (comment/uncomment selected lines) Ctrl + Shift + / (reflow comments) Ctrl + Shift + O (View code outline) Ctrl + Shift + B (build package, website or book) Ctrl + Shift + M (pipe) Alt + Shift + K to see all shortcuts… Panels (everything is integrated, including Git and a terminal) Interactive data importation from files and connections (see this webinar) Use code diagnostics: R Projects: Meaningful structure in one folder The working directory automatically switches to the project’s folder File tab displays the associated files and folders in the project History of R commands and open files Any settings associated with the project, such as Git settings, are loaded. Note that a set-up.R or even a .Rprofile file in the project’s root directory enable project-specific settings to be loaded each time people work on the project. The only two things that make @JennyBryan 攼㹤愼㸰戼㹤攼㹤戼㸸愼㸴攼㹤愼㸰戼㹤攼㹤戼㸸愼㸰攼㹤愼㸰戼㹥攼㹤戼㸴愼㹦. Instead use projects + here::here() #rstats pic.twitter.com/GwxnHePL4n — Hadley Wickham (@hadleywickham) December 11 2017 Read more at https://www.tidyverse.org/articles/2017/12/workflow-vs-script/ and also see chapter Efficient set-up of book Efficient R programming. 2.4 Getting help 2.4.1 Help yourself, learn how to debug A basic solution is to print everything, but it usually does not work well on complex problems. A convenient solution to see all the variables’ states in your code is to place some browser() anywhere you want to check the variables’ states. Learn more with this book chapter, this other book chapter, this webinar and this RStudio article. 2.4.2 External help Can’t remember useful functions? Use cheat sheets. You can search for specific R stuff on https://rseek.org/. You should also read documentations carefully. If you’re using a package, search for vignettes and a GitHub repository. You can also use Stack Overflow. The most common use of Stack Overflow is when you have an error or a question, you Google it, and most of the times the first links are Q/A on Stack Overflow. You can ask questions on Stack Overflow (using the tag r). You need to make a great R reproducible example if you want your question to be answered. Most of the times, while making this reproducible example, you will find the answer to your problem. Join the R-help mailing list. Sign up to get the daily digest and scan it for questions that interest you. 2.5 Keeping up to date With over 10,000 packages on CRAN it is hard to keep up with the constantly changing landscape. R-Bloggers is an R focused blog aggregation site with dozens of posts per day. Check it out. 2.6 Exercises See these RStudio Tips &amp; Tricks or these and find one that looks interesting and practice it all week. Create an R Project for this class. Create the following directories in your project (tip sheet?) Bonus points if you can do it from R and not RStudio or Windows Explorer Double Bonus points if you can make it a function. Read Chapters 1-3 of the Tidyverse Style Guide Copy one of your R scripts into your R directory. (Bonus points if you can do it from R and not RStudio or Windows Explorer) Apply the style guide to your code. Apply the “Rule of 3” Create variables as needed Identify code that is used 3 or more times to make functions Identify code that would be useful in 3 or more projects to integrate into a package. Read how to make a great R reproducible example This is sometimes called the DRY principle, or Don’t Repeat Yourself.↩ "],
["baser-rbasics.html", "3 R Basics 3.1 Assignment Operators 3.2 Objects 3.3 Comparision 3.4 Logical and sets 3.5 Control Structures 3.6 Vectorization &amp; Recycling 3.7 Function Basics 3.8 Environments &amp; Scoping 3.9 Exercises", " 3 R Basics Here is a quick overview of the basics. Next we’ll dive deep into R’s basic data structures and then how to subset these data structures. This will give us a good overview of base R and the background needed to dive into R for Data Science. The three most important functions in R ?, ??, and str: ?topic provides access to the documentation for topic. ??topic searches the documentation for topic. str displays the structure of an R object in human readable form. See this vocabulary list for a good starting point on the basics functions in base R and some important libraries. A book to learn the basics is R Programming for Data Science In R there three basic constructs2; objects, functions, and environments. 3.1 Assignment Operators We saw this is Coding Style. Use &lt;- for assignment and use = for parameters. While you can use = for assignment it is generally considered bad practice. 3.2 Objects 3.2.1 Vector You create a vector with c. These have to be the same data type. v &lt;- c(&quot;my&quot;, &quot;first&quot;, &quot;vector&quot;) v #&gt; [1] &quot;my&quot; &quot;first&quot; &quot;vector&quot; # length of our vector length(v) #&gt; [1] 3 There are several shortcut functions for common vector creation. # create an ordered sequence 2:10 #&gt; [1] 2 3 4 5 6 7 8 9 10 9:3 #&gt; [1] 9 8 7 6 5 4 3 # generate regular sequences seq(1, 20, by = 3) #&gt; [1] 1 4 7 10 13 16 19 # replicate a number n times rep(3, times = 4) #&gt; [1] 3 3 3 3 # arguments are generally vectorized rep(1:3, times = 3:1) #&gt; [1] 1 1 1 2 2 3 # common mistake using 1:length(n) in loops # but if n = 0 1:0 #&gt; [1] 1 0 # use seq_len(n) instead and the loop won&#39;t execute seq_len(0) #&gt; integer(0) # another common mistake n &lt;- 6 1:n+1 # is (1:n) + 1, so 2:(n + 1) #&gt; [1] 2 3 4 5 6 7 1:(n+1) # usually what is meant #&gt; [1] 1 2 3 4 5 6 7 seq_len(n+1) # a better way #&gt; [1] 1 2 3 4 5 6 7 3.2.2 Matrix Matrices are 2D vectors, with all elements of the same type. Generally used for mathematics. # fill in column order (default) matrix(1:12, nrow = 3) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 4 7 10 #&gt; [2,] 2 5 8 11 #&gt; [3,] 3 6 9 12 # fill in row order matrix(1:12, nrow = 3, byrow = TRUE) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 2 3 4 #&gt; [2,] 5 6 7 8 #&gt; [3,] 9 10 11 12 # can also specify the number of columns instead matrix(1:12, ncol = 3) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 You find the dimensions of a matrix with nrow, ncol, and dim m &lt;- matrix(1:12, ncol = 3) dim(m) #&gt; [1] 4 3 nrow(m) #&gt; [1] 4 ncol(m) #&gt; [1] 3 3.2.3 List A list is a generic vector containing other objects. These do NOT have to be the same type or the same length. s &lt;- c(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;, &quot;ee&quot;) b &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) # x contains copies of n, s, b and our matrix from above x &lt;- list(n = c(2, 3, 5) , s, b, 3, m) x #&gt; $n #&gt; [1] 2 3 5 #&gt; #&gt; [[2]] #&gt; [1] &quot;aa&quot; &quot;bb&quot; &quot;cc&quot; &quot;dd&quot; &quot;ee&quot; #&gt; #&gt; [[3]] #&gt; [1] TRUE FALSE TRUE FALSE FALSE #&gt; #&gt; [[4]] #&gt; [1] 3 #&gt; #&gt; [[5]] #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 # length gives you length of the list not the elements in the list length(x) #&gt; [1] 5 We’ll discuss lists in detail in the next chapter. 3.2.4 Data frame A data frame is a list with each vector of the same length. This is the main data structure used and is analogous to a data set in SAS. While these look like matrices they behave very different. df = data.frame(n = c(2, 3, 5), s = c(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;), b = c(TRUE, FALSE, TRUE), y = v ) # df is a data frame df #&gt; n s b y #&gt; 1 2 aa TRUE my #&gt; 2 3 bb FALSE first #&gt; 3 5 cc TRUE vector # dimensions dim(df) #&gt; [1] 3 4 nrow(df) #&gt; [1] 3 ncol(df) #&gt; [1] 4 length(df) #&gt; [1] 4 We’ll discuss data frames in greater detail in the next chapter. 3.3 Comparision TABLE 3.1: Logical Operators Operator Description &gt; greater than &gt;= greater than or equal to &lt; less than &lt;= less than or equal to == exactly equal to != not equal to v &lt;- 1:12 v[v &gt; 9] #&gt; [1] 10 11 12 Equality can be tricky to test for since real numbers can’t be expressed exactly in computers. x &lt;- sqrt(2) (y &lt;- x^2) #&gt; [1] 2 y == 2 #&gt; [1] FALSE print(y, digits = 20) #&gt; [1] 2.0000000000000004 all.equal(y, 2) ## equality with some tolerance #&gt; [1] TRUE all.equal(y, 3) #&gt; [1] &quot;Mean relative difference: 0.5&quot; isTRUE(all.equal(y, 3)) ## if you want a boolean, use isTRUE() #&gt; [1] FALSE 3.4 Logical and sets x &lt;- c(TRUE, FALSE) df &lt;- data.frame(expand.grid(x, x)) names(df) &lt;- c(&quot;x&quot;, &quot;y&quot;) df$and &lt;- df$x &amp; df$y # logical and df$or &lt;- df$x | df$y # logical or df$notx &lt;- !df$x # negation df$xor &lt;- xor(df$x, df$y) # exlusive or df #&gt; x y and or notx xor #&gt; 1 TRUE TRUE TRUE TRUE FALSE FALSE #&gt; 2 FALSE TRUE FALSE TRUE TRUE TRUE #&gt; 3 TRUE FALSE FALSE TRUE FALSE TRUE #&gt; 4 FALSE FALSE FALSE FALSE TRUE FALSE R has two versions of the logical operators &amp; and &amp;&amp; (| and ||). The single version is the vectorized version while the the double version returns a length-one vector. Use the double version in logical control structures (if, for, while, etc). df$x &amp;&amp; df$y # only and the first elements #&gt; [1] TRUE df$x || df$y # only or the first elements #&gt; [1] TRUE This is a common source of bugs in control structures (if, for, while, etc) where you must have a single TRUE / FALSE. = is used for assignment while == is used for comparison. A common bug is to use = instead of == inside a control structure. It also has useful helpers any and all x &lt;- c(FALSE, FALSE, FALSE, TRUE) any(x) #&gt; [1] TRUE all(x) #&gt; [1] FALSE all(!x[1:3]) #&gt; [1] TRUE And also some useful set operations intersect, union, setdiff, setequal x &lt;- 1:5 y &lt;- 3:7 intersect(x, y) # in x and in y #&gt; [1] 3 4 5 union(x, y) # different than c() #&gt; [1] 1 2 3 4 5 6 7 c(x,y) # not a set operation #&gt; [1] 1 2 3 4 5 3 4 5 6 7 setdiff(x, y) # in x but not in y #&gt; [1] 1 2 setdiff(y, x) # in y but not in x #&gt; [1] 6 7 setequal(x, y) #&gt; [1] FALSE z &lt;- 5:1 setequal(x, z) #&gt; [1] TRUE 3.5 Control Structures Control structures allow you to put some “logic” into your R code, rather than just always executing the same R code every time. Control structures allow you to respond to inputs or to features of the data and execute different R expressions accordingly. Commonly used control structures are if and else: testing a condition and acting on it for: execute a loop a fixed number of times while: execute a loop while a condition is true repeat: execute an infinite loop (must break out of it to stop) break: break the execution of a loop next: skip an iteration of a loop 3.5.1 if-else The if-else combination is probably the most commonly used control structure in R (or perhaps any language). This structure allows you to test a condition and act on it depending on whether it’s true or false. For starters, you can just use the if statement. if(&lt;condition&gt;) { # do something } # Continue with rest of code The above code does nothing if the condition is false. If you have an action you want to execute when the condition is false, then you need an else clause. if(&lt;condition&gt;) { # do something } else { # do something else } You can have a series of tests by following the initial if with any number of else ifs. if(&lt;condition1&gt;) { # do something } else if(&lt;condition2&gt;) { # do something different } else { # do something else different } There is also an ifelse function which is vectorized version. It is essentially an if-else wrapped in a for loop so that the condition, and action, is performed on each element in a vector. 3.5.2 for Loops For loops are pretty much the only looping construct that you will need in R. While you may occasionally find a need for other types of loops, in my experience doing data analysis, I’ve found very few situations where a for loop wasn’t sufficient. In R, for loops take an iterator variable and assign it successive values from a sequence or vector. For loops are most commonly used for iterating over the elements of an object (list, vector, etc.) The following three loops all have the similar behavior. x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) for(i in 1:length(x)) { ## Print out each element of &#39;x&#39; print(x[i]) } #&gt; [1] &quot;a&quot; #&gt; [1] &quot;b&quot; #&gt; [1] &quot;c&quot; #&gt; [1] &quot;d&quot; The seq_along() function is commonly used in conjunction with for loops in order to generate an integer sequence based on the length of an object (in this case, the object x). ## Generate a sequence based on length of &#39;x&#39; for(i in seq_along(x)) { print(x[i]) } #&gt; [1] &quot;a&quot; #&gt; [1] &quot;b&quot; #&gt; [1] &quot;c&quot; #&gt; [1] &quot;d&quot; It is not necessary to use an index-type variable. for(letter in x) { print(letter) } #&gt; [1] &quot;a&quot; #&gt; [1] &quot;b&quot; #&gt; [1] &quot;c&quot; #&gt; [1] &quot;d&quot; Nested loops are commonly needed for multidimensional or hierarchical data structures (e.g. matrices, lists). Be careful with nesting though. Nesting beyond 2 to 3 levels often makes it difficult to read/understand the code. If you find yourself in need of a large number of nested loops, you may want to break up the loops by using functions (discussed later). 3.5.3 while Loops While loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop body is executed, the condition is tested again, and so forth, until the condition is false, after which the loop exits. count &lt;- 0 while(count &lt; 10) { print(count) count &lt;- count + 1 } #&gt; [1] 0 #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 #&gt; [1] 6 #&gt; [1] 7 #&gt; [1] 8 #&gt; [1] 9 While loops can potentially result in infinite loops if not written properly. Use with care! Sometimes there will be more than one condition in the test. z &lt;- 5 set.seed(1) while(z &gt;= 3 &amp;&amp; z &lt;= 10) { coin &lt;- rbinom(1, 1, 0.5) if(coin == 1) { ## random walk z &lt;- z + 1 } else { z &lt;- z - 1 } } print(z) #&gt; [1] 2 Conditions are always evaluated from left to right. For example, in the above code, if z were less than 3, the second test would not have been evaluated. 3.5.4 repeat Loops repeat initiates an infinite loop right from the start. These are not commonly used in statistical or data analysis applications but they do have their uses. The only way to exit a repeat loop is to call break. One possible paradigm might be in an iterative algorithm where you may be searching for a solution and you don’t want to stop until you’re close enough to the solution. In this kind of situation, you often don’t know in advance how many iterations it’s going to take to get “close enough” to the solution. x0 &lt;- 1 tol &lt;- 1e-8 repeat { x1 &lt;- computeEstimate() if(abs(x1 - x0) &lt; tol) { ## Close enough? break } else { x0 &lt;- x1 } } The above code will not run since the computeEstimate() function is not defined. I just made it up for the purposes of this demonstration. The loop above is a bit dangerous because there’s no guarantee it will ever stop. You could get in a situation where the values of x0 and x1 oscillate back and forth and never converge. Better to set a hard limit on the number of iterations by using a for loop and then report whether convergence was achieved or not. 3.5.5 next, break While not used very often it’s nice to know about these. next is used to skip an iteration of a loop. for(i in 1:100) { if(i &lt;= 20) { ## Skip the first 20 iterations next } ## Do something here } break is used to exit a loop immediately, regardless of what iteration the loop may be on. for(i in 1:100) { print(i) if(i &gt; 20) { ## Stop loop after 20 iterations break } } 3.5.6 Looping For loops are so common that that R has some functions which implement looping in a compact form to make your life easier. For a more in depth look see this apply is generic: applies a function to a matrix’s rows or columns (or, more generally, to dimensions of an array) lapply is a list apply which acts on a list or vector and returns a list. sapply is a simple lapply but defaults to returning a vector (or matrix) if possible. vapply is a verified apply. This is a sapply with the return object type pre-specified. rapply is a recursive apply for nested lists, i.e. lists within lists tapply is a tagged apply where the tags identify the subsets to apply a function mapply is a multivariate apply for functions that have multiple arguments. Map is a wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list. replicate is a wrapper around sapply for repeated evaluation of an expression # Two dimensional matrix M &lt;- matrix(sample(1:16), 4, 4) M #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 10 9 4 14 #&gt; [2,] 8 12 6 16 #&gt; [3,] 3 2 15 5 #&gt; [4,] 11 7 13 1 # apply min to rows apply(M, 1, min) #&gt; [1] 4 6 2 1 # apply max to columns apply(M, 2, max) #&gt; [1] 11 12 15 16 If you want row/column means or sums for a 2D matrix, be sure to investigate the highly optimized, lightning-quick colMeans, rowMeans, colSums, rowSums. x &lt;- list(a = 1, b = 1:3, c = 10:25) x #&gt; $a #&gt; [1] 1 #&gt; #&gt; $b #&gt; [1] 1 2 3 #&gt; #&gt; $c #&gt; [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 lapply(x, FUN = length) #&gt; $a #&gt; [1] 1 #&gt; #&gt; $b #&gt; [1] 3 #&gt; #&gt; $c #&gt; [1] 16 sapply(x, FUN = length) #&gt; a b c #&gt; 1 3 16 vapply(x, FUN = length, FUN.VALUE = 0L) #&gt; a b c #&gt; 1 3 16 x &lt;- 1:20 y &lt;- factor(rep(letters[1:5], each = 4)) # a vector of the same length as x tapply(x, y, sum) #&gt; a b c d e #&gt; 10 26 42 58 74 # Sums the 1st elements, the 2nd elements, etc. mapply(sum, 1:5, 1:5, 1:5) #&gt; [1] 3 6 9 12 15 # find the mean of 10 random normal variables, 5 times replicate(5, mean(rnorm(10))) #&gt; [1] -0.2258 0.4434 -0.0499 -0.3555 -0.1810 tapply is in a similar spirit to a common data analysis paradigm called split-apply-combine where we split our data set based on a group, apply a function or code to it, and combine the results back together. We will revisit this paradigm in greater detail when we get to R for Data Science. 3.6 Vectorization &amp; Recycling Many operations in R are vectorized, meaning that operations occur in parallel in certain R objects. This allows you to write code that is efficient, concise, and easier to read than in non-vectorized languages. The simplest example is when adding two vectors together. x &lt;- 1:3 y &lt;- 11:13 z &lt;- x + y z #&gt; [1] 12 14 16 In most other languages you would have to do something like z &lt;- numeric(length(x)) for(i in seq_along(x)) { z[i] &lt;- x[i] + y[i] } z #&gt; [1] 12 14 16 We saw a form of vectorization above in the logical operators. x #&gt; [1] 1 2 3 x &gt; 2 #&gt; [1] FALSE FALSE TRUE x[x &gt; 2] #&gt; [1] 3 Matrix operations are also vectorized, making for nice compact notation. This way, we can do element-by-element operations on matrices without having to loop over every element. x &lt;- matrix(1:4, 2, 2) y &lt;- matrix(rep(10, 4), 2, 2) x #&gt; [,1] [,2] #&gt; [1,] 1 3 #&gt; [2,] 2 4 y #&gt; [,1] [,2] #&gt; [1,] 10 10 #&gt; [2,] 10 10 x * y # element-wise multiplication #&gt; [,1] [,2] #&gt; [1,] 10 30 #&gt; [2,] 20 40 x / y # element-wise division #&gt; [,1] [,2] #&gt; [1,] 0.1 0.3 #&gt; [2,] 0.2 0.4 x %*% y # true matrix multiplication #&gt; [,1] [,2] #&gt; [1,] 40 40 #&gt; [2,] 60 60 R also recycles arguments. x &lt;- 1:10 z &lt;- x + .1 # add .1 to each element z #&gt; [1] 1.1 2.1 3.1 4.1 5.1 6.1 7.1 8.1 9.1 10.1 While you usually either want the same length vector or a length one vector. You are not limited to just these options. x &lt;- 1:10 y &lt;- x + c(.1, .2) y #&gt; [1] 1.1 2.2 3.1 4.2 5.1 6.2 7.1 8.2 9.1 10.2 z &lt;- x + c(.1, .2, .3) #&gt; Warning in x + c(0.1, 0.2, 0.3): longer object length is not a multiple of #&gt; shorter object length z #&gt; [1] 1.1 2.2 3.3 4.1 5.2 6.3 7.1 8.2 9.3 10.1 3.6.1 Example One (not so good) way to estimate pi is through Monte-Carlo simulation. Suppose we wish to estimate the value of pi using a Monte-Carlo method. Essentially, we throw darts at the unit square and count the number of darts that fall within the unit circle. We’ll only deal with quadrant one. Thus the \\(Area = \\frac{\\pi}{4}\\) Monte-Carlo pseudo code: Initialize hits = 0 for i in 1:N Generate two random numbers, \\(U_1\\) and \\(U_2\\), between 0 and 1 If \\(U_1^2 + U_2^2 &lt; 1\\), then hits = hits + 1 end for Area estimate = hits / N \\(\\hat{pi} = 4 * Area Estimate\\) pi_naive &lt;- function(N) { hits &lt;- 0 for(i in seq_len(N)) { U1 &lt;- runif(1) U2 &lt;- runif(1) if ((U1^2 + U2^2) &lt; 1) { hits &lt;- hits + 1 } } 4*hits/N } N &lt;- 1e6 system.time(pi_naive(N)) #&gt; user system elapsed #&gt; 3.71 0.01 3.83 That’s a long run time (and bad estimate). Let’s vectorize it. pi_vect &lt;- function(N) { U1 &lt;- runif(N) U2 &lt;- runif(N) hits &lt;- sum(U1^2 + U2^2 &lt; 1) 4*hits/N } system.time(pi_vect(N)) #&gt; user system elapsed #&gt; 0.21 0.00 0.34 That is ~20x speed up. 3.7 Function Basics To understand computations in R, two slogans are helpful: - Everything that exists is an object. - Everything that happens is a function call. – John Chambers Functions are an central part of robust R programming and we will spend a significant amount of time writing functions. Functions in R are “first class objects”, which means that they can be treated much like any other R object. Importantly, Functions can be passed as arguments to other functions. This is very handy for the various apply functions, like lapply() and sapply(). Functions can be nested, so that you can define a function inside of another function If you’re familiar with common language like C, these features might appear a bit strange. However, they are really important in R and can be useful for data analysis. Functions are a means of abstraction. A concept/computation is encapsulated/isolated from the rest with a function. Functions should do one thing, and do it well (compute, or plot, or save, … not all in one go). Side effects: your functions should not have any (unless, of course, that is the main point of that function - plotting, write to disk, …). Functions shouldn’t make any changes in any environment. The only return their output. Do not use global variables. Everything the function needs is being passed as an argument. Function must be self-contained. Function streamline code and process Advice from the R Inferno: Make your functions as simple as possible. Simple has many advantages: Simple functions are likely to be human efficient: they will be easy to understand and to modify. Simple functions are likely to be computer efficient. Simple functions are less likely to be buggy, and bugs will be easier to fix. (Perhaps ironically) simple functions may be more general—thinking about the heart of the matter often broadens the application. Functions can be Correct. An error occurs that is clearly identified. An obscure error occurs. An incorrect value is returned. We like category 1. Category 2 is the right behavior if the inputs do not make sense, but not if the inputs are sensible. Category 3 is an unpleasant place for your users, and possibly for you if the users have access to you. Category 4 is by far the worst place to be - the user has no reason to believe that anything is wrong. Steer clear of category 4. 3.7.1 Your First Function All R functions have three parts: the body(), the code inside the function. the formals(), the list of arguments which controls how you can call the function. the environment(), the “map” of the location of the function’s variables. When you print a function in R, it shows you these three important components. If the environment isn’t displayed, it means that the function was created in the global environment. myadd &lt;- function(x, y) { cat(paste0(&quot;x = &quot;, x, &quot;\\n&quot;)) cat(paste0(&quot;y = &quot;, y, &quot;\\n&quot;)) x + y } myadd(1, 3) # arguments by position #&gt; x = 1 #&gt; y = 3 #&gt; [1] 4 myadd(x = 1, y = 3) # arguments by name #&gt; x = 1 #&gt; y = 3 #&gt; [1] 4 myadd(y = 3, x = 1) # name order doesn&#39;t matter #&gt; x = 1 #&gt; y = 3 #&gt; [1] 4 The body of the function is everything between the { }. Note this does the computation AND returns the result. x and y are the arguments to the function. the environment this function lives in is the global environment. (We’ll discuss environments more in the next section.) Even though it’s legal, I don’t recommend messing around with the order of the arguments too much, since it can lead to some confusion. You can also specify default values for your arguments. Default values should be the values most often used. rnorm uses the default of mean = 0 and sd = 1. We usually want to sample from the standard normal distribution, but we are not forced to. myadd2 &lt;- function(x = 3, y = 0){ cat(paste0(&quot;x = &quot;, x, &quot;\\n&quot;)) cat(paste0(&quot;y = &quot;, y, &quot;\\n&quot;)) x + y } myadd2() # use the defaults #&gt; x = 3 #&gt; y = 0 #&gt; [1] 3 myadd2(x = 1) #&gt; x = 1 #&gt; y = 0 #&gt; [1] 1 myadd2(y = 1) #&gt; x = 3 #&gt; y = 1 #&gt; [1] 4 myadd2(x = 1, y = 1) #&gt; x = 1 #&gt; y = 1 #&gt; [1] 2 By default the last line of the function is returned. Thus, there is no reason to explicitly call return, unless you are returning from the function early. Inside functions use stop to return error messages, warning to return warning messages, and message to print a message to the console. f &lt;- function(age) { if (age &lt; 0) { stop(&quot;age must be a positive number&quot;) } if (age &lt; 18) { warning(&quot;Check your data. We only care about adults.&quot;) } message(paste0(&quot;Your person is &quot;, age, &quot; years old&quot;)) invisible() } f(-10) #&gt; Error in f(-10): age must be a positive number f(10) #&gt; Warning in f(10): Check your data. We only care about adults. #&gt; Your person is 10 years old f(30) #&gt; Your person is 30 years old 3.7.2 Lazy Evaluation R is lazy. Arguments to functions are evaluated lazily, that is they are evaluated only as needed in the body of the function. In this example, the function f() has two arguments: a and b. f &lt;- function(a, b) { a^2 } f(2) # this works #&gt; [1] 4 f(2, 1) # this does too #&gt; [1] 4 This function never actually uses the argument b, so calling f(2) or f(2, 1) will not produce an error because the 2 gets positionally matched to a. It’s common to write a function that does not use an argument and not notice it simply because R never throws an error. 3.7.3 The ... Argument There is a special argument in R known as the ... argument, which indicate a variable number of arguments that are usually passed on to other functions. The ... argument is often used when extending another function and you don’t want to copy the entire argument list of the original function For example, a custom plotting function may want to make use of the default plot() function along with its entire argument list. The function below changes the default for the type argument to the value type = &quot;l&quot; (the original default was type = &quot;p&quot;). myplot &lt;- function(x, y, type = &quot;l&quot;, ...) { plot(x, y, type = type, ...) ## Pass &#39;...&#39; to &#39;plot&#39; function } The ... argument is also necessary when the number of arguments passed to the function cannot be known in advance. This is clear in functions like paste() and cat(). args(paste) #&gt; function (..., sep = &quot; &quot;, collapse = NULL) #&gt; NULL args(cat) #&gt; function (..., file = &quot;&quot;, sep = &quot; &quot;, fill = FALSE, labels = NULL, #&gt; append = FALSE) #&gt; NULL Because both paste() and cat() print out text to the console by combining multiple character vectors together, it is impossible for those functions to know in advance how many character vectors will be passed to the function by the user. So the first argument to either function is .... One catch with ... is that any arguments that appear after ... on the argument list must be named explicitly and cannot be partially matched or matched positionally. Take a look at the arguments to the paste() function. args(paste) #&gt; function (..., sep = &quot; &quot;, collapse = NULL) #&gt; NULL With the paste() function, the arguments sep and collapse must be named explicitly and in full if the default values are not going to be used. 3.8 Environments &amp; Scoping An environment is a collection of (symbol, value) pairs, i.e. x is a symbol and 3.14 might be its value. Every environment has a parent environment and it is possible for an environment to have multiple “children”. The only environment without a parent is the empty environment. Scoping is the set of rules that govern how R looks up the value of a symbol. In the example below, scoping is the set of rules that R applies to go from the symbol x to its value 10: x &lt;- 10 x #&gt; [1] 10 R has two types of scoping: lexical scoping, implemented automatically at the language level, and dynamic scoping, used in select functions to save typing during interactive analysis. We discuss lexical scoping here because it is intimately tied to function creation. Dynamic scoping is an advanced topic and is discussed in Advanced R. How do we associate a value to a free variable? There is a search process that occurs that goes as follows: If the value of a symbol is not found in the environment in which a function was defined, then the search is continued in the parent environment. The search continues up the sequence of parent environments until we hit the top-level environment; this usually the global environment (workspace) or the namespace of a package. After the top-level environment, the search continues down the search list until we hit the empty environment. If a value for a given symbol cannot be found once the empty environment is arrived at, then an error is thrown. x &lt;- 0 f &lt;- function(x = -1) { x &lt;- 1 y &lt;- 2 c(x, y) } g &lt;- function(x = -1) { y &lt;- 1 c(x, y) } h &lt;- function() { y &lt;- 1 c(x, y) } What do the following return? f() g() h() g(h()) f(g()) g(f()) Unlike most languages you can define a function within a function. This nested function only lives inside the parent function. make.power &lt;- function(n) { pow &lt;- function(x) { x^n } pow } make.power(4) #&gt; function(x) { #&gt; x^n #&gt; } #&gt; &lt;environment: 0x0000000008776ad8&gt; cube &lt;- make.power(3) square &lt;- make.power(2) x &lt;- 1 n &lt;- 2 pow(x=4) #&gt; Error in pow(x = 4): could not find function &quot;pow&quot; 3.9 Exercises Browse this vocabulary list and read the help file for functions that interest you. Re-run the three cases in the For loop section with x &lt;- NULL Vectorization / function practice. We’ll calculate pi using the Gregory-Leibniz series. Mathematicians will be quick to point out that this is a poor way to calculate pi, since the series converges very slowly. But our goal is not calculating pi, our goal is examining the performance benefit that be be achieved using vectorization. Here is a formula for the Gregory-Leibniz series: \\[\\begin{equation} 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\frac{1}{9} - \\frac{1}{11} + \\cdots = \\frac{\\pi}{4} \\end{equation}\\] Here is the Gregory-Leibniz series in summation notation: \\[\\begin{equation} \\sum_{\\text{n}=0}^{\\infty} \\frac{(-1)^n}{2\\cdot n + 1} = \\frac{\\pi}{4} \\end{equation}\\] The straightforward implementation using an R loop would look like this: GL_naive &lt;- function (limit) { p = 0 for (n in 0:limit) { p = (-1)^n/(2 * n + 1) + p } 4*p } N &lt;- 1e7 system.time(pi_est &lt;- GL_naive(N)) #&gt; user system elapsed #&gt; 2.11 0.00 2.12 pi_est #&gt; [1] 3.14 Your task is to vectorize this function. Do not use any looping or apply functions. This one is a bit tricky. Hint: It may be easier to think about it in terms of the series notation and not the summation notation. GL_vect &lt;- function(limit) { # your code here # use only base functions and no looping mechanisms } Technically speaking functions and environments are objects which allows one to do things in R you can’t do in many other languages.↩ "],
["base-r-data-structures.html", "4 Base R Data Structures 4.1 Naming Rules 4.2 Vectors", " 4 Base R Data Structures 4.1 Naming Rules R has strict rules about what constitutes a valid name. A syntactic name must consist of letters3, digits, . and _, and can’t begin with _. Additionally, it can not be one of a list of reserved words like TRUE, NULL, if, and function (see the complete list in ?Reserved). Names that don’t follow these rules are called non-syntactic names, and if you try to use them, you’ll get an error: _abc &lt;- 1 #&gt; Error: unexpected input in &quot;_&quot; if &lt;- 10 #&gt; Error: unexpected assignment in &quot;if &lt;-&quot; While TRUE and FALSE are reserved words T and F are not. However, you can use T and F as logical. If someone assigns either of those a different value you will get a very hard to track down bug. Always spell out the TRUE and FALSE. 4.2 Vectors The most common data structure in R is the vector. R’s vectors can be organised by their dimensionality (1d, 2d, or nd) and whether they’re homogeneous or heterogeneous. This gives rise to the five data types most often used in data analysis: Homogeneous Heterogeneous 1d Atomic vector List 2d Matrix Data frame nd Array Given an object, the best way to understand what data structures it is composed of is to use str(). str() is short for structure and it gives a compact, human readable description of any R data structure. Vectors have three common properties: Type, typeof(), what it is. Length, length(), how many elements it contains. Attributes, attributes(), additional arbitrary metadata. They differ in the types of their elements: all elements of an atomic vector must be the same type, whereas the elements of a list can have different types. is.vector() does not test if an object is a vector. Instead it returns TRUE only if the object is a vector with no attributes apart from names. Use is.atomic(x) || is.list(x) to test if an object is actually a vector. 4.2.1 Atomic Vectors There are many “atomic” types of data: logical, integer, double and character (in this order, see below). There are also raw and complex but they are rarely used. You can’t mix types in an atomic vector (you can in a list). Coercion will automatically occur if you mix types: (a &lt;- FALSE) #&gt; [1] FALSE typeof(a) #&gt; [1] &quot;logical&quot; (b &lt;- 1:10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 typeof(b) #&gt; [1] &quot;integer&quot; c(a, b) ## FALSE is coerced to integer 0 #&gt; [1] 0 1 2 3 4 5 6 7 8 9 10 (c &lt;- 10.5) #&gt; [1] 10.5 typeof(c) #&gt; [1] &quot;double&quot; (d &lt;- c(b, c)) ## coerced to double #&gt; [1] 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 10.5 c(d, &quot;a&quot;) ## coerced to character #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; #&gt; [11] &quot;10.5&quot; &quot;a&quot; 50 &lt; &quot;7&quot; #&gt; [1] TRUE You can force coercion with as.logical, as.integer, as.double, as.numeric, and as.character. Most of the time the coercion rules are straight forward, but not always. x &lt;- c(TRUE, FALSE) typeof(x) #&gt; [1] &quot;logical&quot; as.integer(x) #&gt; [1] 1 0 as.numeric(x) #&gt; [1] 1 0 as.character(x) #&gt; [1] &quot;TRUE&quot; &quot;FALSE&quot; However, coercion is not associative. x &lt;- c(TRUE, FALSE) x2 &lt;- as.integer(x) x3 &lt;- as.numeric(x2) as.character(x3) #&gt; [1] &quot;1&quot; &quot;0&quot; What would you expect this to return? x &lt;- c(TRUE, FALSE) as.integer(as.character(x)) You can test for an “atomic” types of data with: is.logical, is.integer, is.double, is.numeric4, and is.character. x &lt;- c(TRUE, FALSE) is.logical(x) #&gt; [1] TRUE is.integer(x) #&gt; [1] FALSE What would you expect these to return? x &lt;- 2 is.integer(x) is.numeric(x) is.double(x) Missing values are specified with NA, which is a logical vector of length 1. NA will always be coerced to the correct type if used inside c(), or you can create NAs of a specific type with NA_real_ (a double vector), NA_integer_ and NA_character_. 4.2.2 Lists Lists are different from atomic vectors because their elements can be of any type, including other lists. Lists can contain complex objects so it’s not possible to pick one visual style that works for every list. You construct lists by using list() instead of c(): x &lt;- list(1:3, &quot;a&quot;, c(TRUE, FALSE, TRUE), c(2.3, 5.9)) str(x) #&gt; List of 4 #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;a&quot; #&gt; $ : logi [1:3] TRUE FALSE TRUE #&gt; $ : num [1:2] 2.3 5.9 Lists are sometimes called recursive vectors, because a list can contain other lists. This makes them fundamentally different from atomic vectors. x &lt;- list(list(list(list(1)))) str(x) #&gt; List of 1 #&gt; $ :List of 1 #&gt; ..$ :List of 1 #&gt; .. ..$ :List of 1 #&gt; .. .. ..$ : num 1 is.recursive(x) #&gt; [1] TRUE c() will combine several lists into one. If given a combination of atomic vectors and lists, c() will coerce the vectors to lists before combining them. Compare the results of list() and c(): l1 &lt;- list(1, 2) c1 &lt;- c(3, 4) x &lt;- list(l1, c1) y &lt;- c(l1, c1) str(x) #&gt; List of 2 #&gt; $ :List of 2 #&gt; ..$ : num 1 #&gt; ..$ : num 2 #&gt; $ : num [1:2] 3 4 str(y) #&gt; List of 4 #&gt; $ : num 1 #&gt; $ : num 2 #&gt; $ : num 3 #&gt; $ : num 4 The typeof() a list is list. You can test for a list with is.list() and coerce to a list with as.list(). You can turn a list into an atomic vector with unlist(). If the elements of a list have different types, unlist() uses the same coercion rules as c(). Lists are used to build up many of the more complicated data structures in R. For example, both data frames (described in data frames) and linear models objects (as produced by lm()) are lists 4.2.3 NULL Closely related to vectors is NULL, a singleton object often used to represent a vector of length 0. NULL is different than NA. For a good explanation of the differences see this blog post. 4.2.4 Attributes All objects can have arbitrary additional attributes, used to store metadata about the object. Attributes can be thought of as a named list5 (with unique names). Attributes can be accessed individually with attr() or all at once (as a list) with attributes(). a &lt;- 1:3 attr(a, &quot;createdBy&quot;) &lt;- &quot;Brian Davis&quot; attr(a, &quot;version&quot;) &lt;- 1.0 attr(a, &quot;z&quot;) &lt;- list(list()) a #&gt; [1] 1 2 3 #&gt; attr(,&quot;createdBy&quot;) #&gt; [1] &quot;Brian Davis&quot; #&gt; attr(,&quot;version&quot;) #&gt; [1] 1 #&gt; attr(,&quot;z&quot;) #&gt; attr(,&quot;z&quot;)[[1]] #&gt; list() attributes(a) #&gt; $createdBy #&gt; [1] &quot;Brian Davis&quot; #&gt; #&gt; $version #&gt; [1] 1 #&gt; #&gt; $z #&gt; $z[[1]] #&gt; list() str(attributes(a)) #&gt; List of 3 #&gt; $ createdBy: chr &quot;Brian Davis&quot; #&gt; $ version : num 1 #&gt; $ z :List of 1 #&gt; ..$ : list() The structure() function returns a new object with modified attributes. Care must be taken with attributes since, by default, most attributes are lost when modifying a vector. attributes(a[1]) #&gt; NULL attributes(sum(a)) #&gt; NULL The only attributes not lost are the three most important: Names, a character vector giving each element a name. Dimensions, used to turn vectors into matrices and arrays. Class, used to implement the S3 object system. Each of these attributes has a specific accessor function to get and set values. When working with these attributes, use names(x), dim(x), and class(x), not attr(x, &quot;names&quot;), attr(x, &quot;dim&quot;), and attr(x, &quot;class&quot;). 4.2.4.1 Names You can name a vector in a couple6 ways: When creating it: x &lt;- c(a = 1, b = 2, c = 3). By modifying an existing vector in place: x &lt;- 1:3; names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;). Named vectors are a great way to make an easy, human readable look up table. We will see this use case extensively when we get to data visualizations. 4.2.5 Factors One important use of attributes is to define factors. A factor is a vector that can contains only predefined values, and is used to store categorical data. Factors are built on top of integer vectors using two attributes: the class, “factor”, which makes them behave differently from regular integer vectors, and the levels, which defines the set of allowed values. Factors can also have labels which effect how the factors are displayed. By default the labels are the same as the levels. The order of the levels of a factor can be set using the levels argument to factor(). This can be important in linear modelling because the first level is used as the baseline level. This feature can also be used to customize order in plots that include factors, since by default factors are plotted in the order of their levels. Labels are also useful in plotting where you want the displayed text to be different than the underlying representation. Factors are useful when you know the possible values a variable may take, even if you don’t see all values in a given data set. Using a factor instead of a character vector makes it obvious when some groups contain no observations: gender_char &lt;- c(&quot;m&quot;, &quot;m&quot;, &quot;m&quot;) gender_factor &lt;- factor(gender_char, levels = c(&quot;m&quot;, &quot;f&quot;)) gender_char #&gt; [1] &quot;m&quot; &quot;m&quot; &quot;m&quot; table(gender_char) #&gt; gender_char #&gt; m #&gt; 3 gender_factor #&gt; [1] m m m #&gt; Levels: m f table(gender_factor) #&gt; gender_factor #&gt; m f #&gt; 3 0 # See the underlying representation of a factor unclass(gender_factor) #&gt; [1] 1 1 1 #&gt; attr(,&quot;levels&quot;) #&gt; [1] &quot;m&quot; &quot;f&quot; gender_factor2 &lt;- factor(gender_char, levels = c(&quot;m&quot;, &quot;f&quot;), labels = c(&quot;Male&quot;, &quot;Female&quot;)) gender_factor2 #&gt; [1] Male Male Male #&gt; Levels: Male Female table(gender_factor2) #&gt; gender_factor2 #&gt; Male Female #&gt; 3 0 # See the underlying representation of a factor unclass(gender_factor2) #&gt; [1] 1 1 1 #&gt; attr(,&quot;levels&quot;) #&gt; [1] &quot;Male&quot; &quot;Female&quot; While factors look like (and often behave like) character vectors, they are actually integers. Be careful when treating them like strings. Some string methods (like gsub() and grepl()) will coerce factors to strings, while others (like nchar()) will throw an error, and still others (like c()) will use the underlying integer values. For this reason, it is best to explicitly convert factors to character vectors if you need string-like behavior. Unfortunately, many base R functions (like read.csv() and data.frame()) automatically convert character vectors to factors. This is sub-optimal, because there’s no way for those functions to know the set of all possible levels or their optimal order. Instead, use the argument stringsAsFactors = FALSE to suppress this behavior, and then manually convert character vectors to factors using your knowledge of the data only when you need the behavior of factors. Factors tend to be most useful in data visualization and table creations where you want to report all categories but some categories may not be present in your data, or when you want to order the categories in something other than the default ordering. We will revisit factors and there usefulness later when we study the tidyverse and in particular the forcats package. 4.2.6 Matrices and arrays Adding a dim attribute to an atomic vector allows it to behave like a multi-dimensional array. A special case of the array is the matrix, which has two dimensions. Matrices are used commonly as part of the mathematical machinery of statistics. Arrays are much rarer, but worth being aware of. Matrices and arrays are created with matrix() and array(), or by using the assignment form of dim(): # Two scalar arguments to specify rows and columns a &lt;- matrix(1:12, ncol = 3, nrow = 4) a #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 # One vector argument to describe all dimensions b &lt;- array(1:12, c(2, 3, 2)) b #&gt; , , 1 #&gt; #&gt; [,1] [,2] [,3] #&gt; [1,] 1 3 5 #&gt; [2,] 2 4 6 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] [,3] #&gt; [1,] 7 9 11 #&gt; [2,] 8 10 12 # You can also modify an object in place by setting dim() vec &lt;- 1:12 vec #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 class(vec) #&gt; [1] &quot;integer&quot; dim(vec) &lt;- c(3, 4) vec #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1 4 7 10 #&gt; [2,] 2 5 8 11 #&gt; [3,] 3 6 9 12 class(vec) #&gt; [1] &quot;matrix&quot; dim(vec) &lt;- c(3, 2, 2) vec #&gt; , , 1 #&gt; #&gt; [,1] [,2] #&gt; [1,] 1 4 #&gt; [2,] 2 5 #&gt; [3,] 3 6 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] #&gt; [1,] 7 10 #&gt; [2,] 8 11 #&gt; [3,] 9 12 class(vec) #&gt; [1] &quot;array&quot; length() and names() have high-dimensional generalizations: length() generalizes to nrow() and ncol() for matrices, and dim() for arrays. names() generalizes to rownames() and colnames() for matrices, and dimnames(), a list of character vectors, for arrays. c() generalizes to cbind() and rbind() for matrices, and to abind::abind() for arrays. You can transpose a matrix with t(); the generalized equivalent for arrays is aperm(). You can test if an object is a matrix or array using is.matrix() and is.array(), or by looking at the length of the dim(). as.matrix() and as.array() make it easy to turn an existing vector into a matrix or array. Vectors are not the only 1-dimensional data structure. You can have matrices with a single row or single column, or arrays with a single dimension. They may print similarly, but will behave differently. The differences aren’t too important, but it’s useful to know they exist in case you get strange output from a function (tapply() is a frequent offender). As always, use str() to reveal the differences. Matrices and arrays are most useful for mathematical calculations (particularly when fitting models); lists and data frames are a better fit for most other programming tasks in R. 4.2.7 Data Frames A data frame is the most common way of storing data in R, and if used systematically makes data analysis easier. Under the hood, a data frame is a list of equal-length vectors. This makes it a 2-dimensional structure, so it shares properties of both the matrix and the list. This means that a data frame has names(), colnames(), and rownames(), although names() and colnames() are the same thing. The length() of a data frame is the length of the underlying list and so is the same as ncol(); nrow() gives the number of rows. You can subset a data frame like a 1d structure (where it behaves like a list), or a 2d structure (where it behaves like a matrix), we will discuss this further when we discuss subsetting. 4.2.7.1 Creation You create a data frame using data.frame(), which takes named vectors as input: df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 2 3 Beware data.frame()’s default behavior which turns strings into factors. Use stringsAsFactors = FALSE to suppress this behavior. df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; Because a data.frame is an S3 class, its type reflects the underlying vector used to build it: the list. typeof(df) #&gt; [1] &quot;list&quot; 4.2.7.2 Testing and coercion Because a data.frame is an S3 class, its type reflects the underlying vector used to build it: the list. To check if an object is a data frame, use is.data.frame(): is.data.frame(df) #&gt; [1] TRUE You can coerce an object to a data frame with as.data.frame(): A vector will create a one-column data frame. A list will create one column for each element; it’s an error if they’re not all the same length. A matrix will create a data frame with the same number of columns and rows as the matrix. The automatic coercion that causes the most problems is if you select a single column of a data.frame. R will coerce the column to an atomic vector, which generally is not what you want7. (x1 &lt;- df[, &quot;y&quot;]) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; str(x1) #&gt; chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; (x2 &lt;- df[, &quot;y&quot;, drop = FALSE]) #&gt; y #&gt; 1 a #&gt; 2 b #&gt; 3 c str(x2) #&gt; &#39;data.frame&#39;: 3 obs. of 1 variable: #&gt; $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; 4.2.7.3 Combining data frames You can combine data frames using cbind() and rbind(): cbind(df, data.frame(z = 3:1)) #&gt; x y z #&gt; 1 1 a 3 #&gt; 2 2 b 2 #&gt; 3 3 c 1 rbind(df, data.frame(x = 10, y = &quot;z&quot;)) #&gt; x y #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c #&gt; 4 10 z When combining column-wise, the number of rows must match, but row names are ignored. When combining row-wise, both the number and names of columns must match. It’s a common mistake to try and create a data frame by cbind()ing vectors together. This is unlikely to do what you want because cbind() will create a matrix unless one of the arguments is already a data frame. Instead use data.frame() directly: # This is always a mistake bad &lt;- data.frame(cbind(a = 1:2, b = c(&quot;a&quot;, &quot;b&quot;))) str(bad) #&gt; &#39;data.frame&#39;: 2 obs. of 2 variables: #&gt; $ a: Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 2 #&gt; $ b: Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 2 good &lt;- data.frame(a = 1:2, b = c(&quot;a&quot;, &quot;b&quot;)) str(good) #&gt; &#39;data.frame&#39;: 2 obs. of 2 variables: #&gt; $ a: int 1 2 #&gt; $ b: Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 2 4.2.7.4 List and matrix columns Since a data frame is a list of vectors, it is possible for a data frame to have a column that is a list. This is a powerful technique because a list can contain any other R object. This means that you can have a column of data frames, or model objects, or even functions! We will see this again when we discuss tidy data. df &lt;- data.frame(x = 1:3) df$y &lt;- list(1:2, 1:3, 1:4) df #&gt; x y #&gt; 1 1 1, 2 #&gt; 2 2 1, 2, 3 #&gt; 3 3 1, 2, 3, 4 However, when a list is given to data.frame(), it tries to put each item of the list into its own column, so this fails: data.frame(x = 1:3, y = list(1:2, 1:3, 1:4)) #&gt; Error in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 2, 3, 4 A workaround is to use I(), which causes data.frame() to treat the list as one unit: dfl &lt;- data.frame(x = 1:3, y = I(list(1:2, 1:3, 1:4))) str(dfl) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y:List of 3 #&gt; ..$ : int 1 2 #&gt; ..$ : int 1 2 3 #&gt; ..$ : int 1 2 3 4 #&gt; ..- attr(*, &quot;class&quot;)= chr &quot;AsIs&quot; I() adds the AsIs class to its input, but this can usually be safely ignored. Similarly, it’s also possible to have a column of a data frame that’s a matrix or array, as long as the number of rows matches the data frame: dfm &lt;- data.frame(x = 1:3 * 10, y = I(matrix(1:9, nrow = 3))) str(dfm) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: num 10 20 30 #&gt; $ y: &#39;AsIs&#39; int [1:3, 1:3] 1 2 3 4 5 6 7 8 9 Use list and array columns with caution. Many functions that work with data frames assume that all columns are atomic vectors, and the printed display can be confusing. dfl[2, ] #&gt; x y #&gt; 2 2 1, 2, 3 dfm[2, ] #&gt; x y.1 y.2 y.3 #&gt; 2 20 2 5 8 Surprisingly, what constitutes a letter is determined by your current locale. That means that the syntax of R code actually differs from computer to computer, and it’s possible for a file that works on one computer to not even parse on another!↩ is.numeric() is a general test for the “numberliness” of a vector and returns TRUE for both integer and double vectors. It is not a specific test for double vectors, which are often called numeric.↩ The reality is a little more complicated: attributes are actually stored in something called pairlists, which can you learn more about in Advanced R↩ There are a couple less common ways. See Advanced R↩ We’ll revisit this when we get into R for Data Science and discuss tibbles↩ "],
["subsetting.html", "5 Subsetting 5.1 Selecting multiple elements 5.2 Selecting a single elements 5.3 Subsetting and assignment 5.4 Applications 5.5 Exercises", " 5 Subsetting R’s subsetting operators are powerful and fast. Mastery of subsetting allows you to succinctly express complex operations in a way that few other languages can match. Subsetting can be hard to learn because you need to master a number of interrelated concepts: The three subsetting operators [ select multiple elements [[, and $ select a single element The six types of subsetting. Positive integers return elements at the specified positions Negative integers omit elements at the specified positions Logical vectors select elements where the corresponding logical value is TRUE Nothing returns the original object. Zero returns a zero-length object (This is not something you usually do on purpose) Character vectors to return elements with matching names. Important differences in behavior for different objects (e.g., vectors, lists, factors, matrices, and data frames). The use of subsetting in conjunction with assignment. It’s easiest to learn how subsetting works for atomic vectors, and then how it generalizes to higher dimensions and other more complicated objects. 5.1 Selecting multiple elements There is one accessor for selecting multiple elements [. 5.1.1 Atomic vectors Let’s explore the different types of subsetting with a simple vector, x. x &lt;- c(2.1, 4.2, 3.3, 5.4) Note that the number after the decimal point gives the original position in the vector. There are five things that you can use to subset a vector. Positive integers return elements at the specified positions x[c(3, 1)] #&gt; [1] 3.3 2.1 # order returns an index x[order(x)] #&gt; [1] 2.1 3.3 4.2 5.4 # Duplicated indices yield duplicated values x[c(1, 1)] #&gt; [1] 2.1 2.1 # Real numbers are silently truncated (not rounded) to integers x[c(2.1, 2.9)] #&gt; [1] 4.2 4.2 Negative integers omit elements at the specified positions x[-c(3, 1)] #&gt; [1] 4.2 5.4 You can’t mix positive and negative integers in a single subset. x[c(-1, 2)] #&gt; Error in x[c(-1, 2)]: only 0&#39;s may be mixed with negative subscripts Logical vectors select elements where the corresponding logical value is TRUE. This is probably the most useful type of subsetting because you write the expression that creates the logical vector: x[c(TRUE, TRUE, FALSE, FALSE)] #&gt; [1] 2.1 4.2 x[x &gt; 3] #&gt; [1] 4.2 3.3 5.4 If the logical vector is shorter than the vector being subsetted, it will be recycled to be the same length. x[c(TRUE, FALSE)] #&gt; [1] 2.1 3.3 # Equivalent to x[c(TRUE, FALSE, TRUE, FALSE)] #&gt; [1] 2.1 3.3 A missing value in the index always yields a missing value in the output. x[c(TRUE, TRUE, NA, FALSE)] #&gt; [1] 2.1 4.2 NA Nothing returns the original vector. This is not useful for vectors but is very useful for matrices, data frames, and arrays. It can also be useful in conjunction with assignment. x[] #&gt; [1] 2.1 4.2 3.3 5.4 Zero returns a zero-length vector. This is not something you usually do on purpose, but it can be helpful for generating test data and testing corner cases of functions. x[0] #&gt; numeric(0) If the vector is named, you can also use: Character vectors to return elements with matching names. (y &lt;- setNames(x, letters[1:4])) #&gt; a b c d #&gt; 2.1 4.2 3.3 5.4 # subsetting by name y[c(&quot;d&quot;, &quot;c&quot;, &quot;a&quot;)] #&gt; d c a #&gt; 5.4 3.3 2.1 # Like integer indices, you can repeat indices y[c(&quot;a&quot;, &quot;a&quot;, &quot;a&quot;)] #&gt; a a a #&gt; 2.1 2.1 2.1 # When subsetting with [ names are always matched exactly z &lt;- c(abc = 1, def = 2) z[c(&quot;a&quot;, &quot;d&quot;)] #&gt; &lt;NA&gt; &lt;NA&gt; #&gt; NA NA 5.1.2 Matrices and Arrays You can subset higher-dimensional structures in three ways: With multiple vectors. With a single vector. With a matrix. The most common way of subsetting matrices (2d) and arrays (&gt;2d) is a simple generalization of 1d subsetting: you supply a 1d index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns. a &lt;- matrix(1:9, nrow = 3) colnames(a) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) a[1:2, ] #&gt; A B C #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 a[c(TRUE, FALSE, TRUE), c(&quot;B&quot;, &quot;A&quot;)] #&gt; B A #&gt; [1,] 4 1 #&gt; [2,] 6 3 a[2:3, -2] #&gt; A C #&gt; [1,] 2 8 #&gt; [2,] 3 9 By default, [ will simplify the results to the lowest possible dimensionality. See below how to avoid this behavior. Because matrices and arrays are implemented as vectors with special attributes, you can subset them with a single vector. In that case, they will behave like a vector. Arrays in R are stored in column-major order: (vals &lt;- outer(1:5, 1:5, FUN = &quot;paste&quot;, sep = &quot;,&quot;)) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] &quot;1,1&quot; &quot;1,2&quot; &quot;1,3&quot; &quot;1,4&quot; &quot;1,5&quot; #&gt; [2,] &quot;2,1&quot; &quot;2,2&quot; &quot;2,3&quot; &quot;2,4&quot; &quot;2,5&quot; #&gt; [3,] &quot;3,1&quot; &quot;3,2&quot; &quot;3,3&quot; &quot;3,4&quot; &quot;3,5&quot; #&gt; [4,] &quot;4,1&quot; &quot;4,2&quot; &quot;4,3&quot; &quot;4,4&quot; &quot;4,5&quot; #&gt; [5,] &quot;5,1&quot; &quot;5,2&quot; &quot;5,3&quot; &quot;5,4&quot; &quot;5,5&quot; vals[c(4, 15)] #&gt; [1] &quot;4,1&quot; &quot;5,3&quot; This behavior allows you to replace all missing values in one line. # make a few values missing vals[sample(1:25, 5)] &lt;- NA_character_ vals #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] &quot;1,1&quot; &quot;1,2&quot; NA &quot;1,4&quot; &quot;1,5&quot; #&gt; [2,] NA &quot;2,2&quot; &quot;2,3&quot; &quot;2,4&quot; &quot;2,5&quot; #&gt; [3,] &quot;3,1&quot; &quot;3,2&quot; &quot;3,3&quot; &quot;3,4&quot; &quot;3,5&quot; #&gt; [4,] NA NA &quot;4,3&quot; &quot;4,4&quot; NA #&gt; [5,] &quot;5,1&quot; &quot;5,2&quot; &quot;5,3&quot; &quot;5,4&quot; &quot;5,5&quot; # replace missing values with &quot;missing&quot; vals[is.na(vals)] &lt;- &quot;missing&quot; vals #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] &quot;1,1&quot; &quot;1,2&quot; &quot;missing&quot; &quot;1,4&quot; &quot;1,5&quot; #&gt; [2,] &quot;missing&quot; &quot;2,2&quot; &quot;2,3&quot; &quot;2,4&quot; &quot;2,5&quot; #&gt; [3,] &quot;3,1&quot; &quot;3,2&quot; &quot;3,3&quot; &quot;3,4&quot; &quot;3,5&quot; #&gt; [4,] &quot;missing&quot; &quot;missing&quot; &quot;4,3&quot; &quot;4,4&quot; &quot;missing&quot; #&gt; [5,] &quot;5,1&quot; &quot;5,2&quot; &quot;5,3&quot; &quot;5,4&quot; &quot;5,5&quot; You can also subset higher-dimensional data structures with an integer matrix (or, if named, a character matrix). Each row in the matrix specifies the location of one value, where each column corresponds to a dimension in the array being subsetted. This means that you use a 2 column matrix to subset a matrix, a 3 column matrix to subset a 3d array, and so on. The result is a vector of values: vals &lt;- outer(1:5, 1:5, FUN = &quot;paste&quot;, sep = &quot;,&quot;) select &lt;- matrix(ncol = 2, byrow = TRUE, c( 1, 1, 3, 1, 2, 4 )) vals[select] #&gt; [1] &quot;1,1&quot; &quot;3,1&quot; &quot;2,4&quot; 5.1.3 Lists Subsetting a list works in the same way as subsetting an atomic vector. Using [ will always return a list; [[ and $, as described below, let you pull out the components of the list. x &lt;- list(1:3, &quot;a&quot;, c(TRUE, FALSE, TRUE), c(2.3, 5.9)) x #&gt; [[1]] #&gt; [1] 1 2 3 #&gt; #&gt; [[2]] #&gt; [1] &quot;a&quot; #&gt; #&gt; [[3]] #&gt; [1] TRUE FALSE TRUE #&gt; #&gt; [[4]] #&gt; [1] 2.3 5.9 x[c(1,4)] #&gt; [[1]] #&gt; [1] 1 2 3 #&gt; #&gt; [[2]] #&gt; [1] 2.3 5.9 5.1.4 Data Frames Data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices. df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df #&gt; x y z #&gt; 1 1 3 a #&gt; 2 2 2 b #&gt; 3 3 1 c # There are two ways to select columns from a data frame # Like a list: df[c(&quot;x&quot;, &quot;z&quot;)] #&gt; x z #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c # Like a matrix df[, c(&quot;x&quot;, &quot;z&quot;)] #&gt; x z #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c # There&#39;s an important difference if you select a single # column: matrix subsetting simplifies by default, list # subsetting does not. str(df[&quot;x&quot;]) #&gt; &#39;data.frame&#39;: 3 obs. of 1 variable: #&gt; $ x: int 1 2 3 str(df[, &quot;x&quot;]) #&gt; int [1:3] 1 2 3 # for row subset like a matrix df[df$x == 2, ] #&gt; x y z #&gt; 2 2 2 b df[c(1, 3), ] #&gt; x y z #&gt; 1 1 3 a #&gt; 3 3 1 c 5.1.5 Preserving dimensionality By default, any subsetting 2d data structures with a single number, single name, or a logical vector containing a single TRUE will simplify the returned output as described below. To preserve the original dimensionality, you must use drop = FALSE For matrices and arrays, any dimensions with length 1 will be dropped: (a &lt;- matrix(1:4, nrow = 2)) #&gt; [,1] [,2] #&gt; [1,] 1 3 #&gt; [2,] 2 4 str(a[1, ]) #&gt; int [1:2] 1 3 str(a[1, , drop = FALSE]) #&gt; int [1, 1:2] 1 3 Data frames with a single column will return just that column: (df &lt;- data.frame(a = 1:2, b = 1:2)) #&gt; a b #&gt; 1 1 1 #&gt; 2 2 2 str(df[, &quot;a&quot;]) #&gt; int [1:2] 1 2 str(df[, &quot;a&quot;, drop = FALSE]) #&gt; &#39;data.frame&#39;: 2 obs. of 1 variable: #&gt; $ a: int 1 2 The default drop = TRUE behavior is a common source of bugs in functions: you check your code with a data frame or matrix with multiple columns, and it works. Six months later you (or someone else) uses it with a single column data frame and it fails with a mystifying error. When writing functions, get in the habit of always using drop = FALSE when subsetting a 2d object. Factor subsetting also has a drop argument, but the meaning it rather different. It controls whether or not levels are preserved (not the dimensionality), and it defaults to FALSE (levels are preserved, not simplified by default). If you find you are using drop = TRUE a lot it’s often a sign that you should be using a character vector instead of a factor. z &lt;- factor(c(&quot;a&quot;, &quot;b&quot;)) z[1] #&gt; [1] a #&gt; Levels: a b z[1, drop = TRUE] #&gt; [1] a #&gt; Levels: a 5.2 Selecting a single elements There are two other subsetting operators: [[ and $. [[ is used for extracting single values, and $ is a useful shorthand for [[ combined with character subsetting. [[ is most important working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor: “If list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.” — @RLangTip, https://twitter.com/RLangTip/status/268375867468681216 Let’s make a simple list and draw it as a train: x &lt;- list(1:3, &quot;a&quot;, 4:6) When extracting a single element, you have two options: you can create a smaller train, or you can extract the contents of a carriage. This is the difference between [ and [[: When extracting multiple elements (or zero!), you have to make a smaller train: Because it can return only a single value, you must use [[ with either a single positive integer or a string. Because data frames are lists of columns, you can use [[ to extract a column from data frames: mtcars[[1]], mtcars[[&quot;cyl&quot;]]. If you use a vector with [[, it will subset recursively: (b &lt;- list(a = list(b = list(c = list(d = 1))))) #&gt; $a #&gt; $a$b #&gt; $a$b$c #&gt; $a$b$c$d #&gt; [1] 1 str(b) #&gt; List of 1 #&gt; $ a:List of 1 #&gt; ..$ b:List of 1 #&gt; .. ..$ c:List of 1 #&gt; .. .. ..$ d: num 1 b[[c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)]] #&gt; [1] 1 # Equivalent to b[[&quot;a&quot;]][[&quot;b&quot;]][[&quot;c&quot;]][[&quot;d&quot;]] #&gt; [1] 1 [[ is crucial for working with lists, but I recommend using it whenever you want your code to clearly express that it’s working with a single value. That frequently arises in for loops, i.e. instead of writing: for (i in 2:length(x)) { out[i] &lt;- fun(x[i], out[i - 1]) } It’s better to write: for (i in 2:length(x)) { out[[i]] &lt;- fun(x[[i]], out[[i - 1]]) } 5.2.1 $ $ is a shorthand operator: x$y is roughly equivalent to x[[&quot;y&quot;]]. It’s often used to access variables in a data frame, as in mtcars$cyl or diamonds$carat. One common mistake with $ is to try and use it when you have the name of a column stored in a variable: var &lt;- &quot;cyl&quot; # Doesn&#39;t work - mtcars$var translated to mtcars[[&quot;var&quot;]] mtcars$var #&gt; NULL # Instead use [[ mtcars[[var]] #&gt; [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 There’s one important difference between $ and [[. $ does partial matching: x &lt;- list(abc = 1, def = 2, ghi = c(4:6)) x$d #&gt; [1] 2 x[[&quot;d&quot;]] #&gt; NULL x[[&quot;def&quot;]] #&gt; [1] 2 It is usually a good idea to NOT use partial matching. It tends to to lead to hard to track down bugs and makes your code much less readable. With auto complete in RStudio it tends not to save any time or keystrokes. 5.2.2 Missing/out of bounds indices TL;DR version use purrr::pluck(), which we will get to in R for Data Science It’s useful to understand what happens with [ and [[ when you use an “invalid” index. The following tables summarize what happen when you subset a logical vector, list, and NULL with an out-of-bounds value (OOB), a missing value (i.e NA_integer_), and a zero-length object (like NULL or logical()) with [ and [[. Each cell shows the result of subsetting the data structure named in the row by the type of index described in the column. I’ve only shown the results for logical vectors, but other atomic vectors behave similarly, returning elements of the same type. row[col] Zero-length OOB Missing Logical logical(0) NA NA List list() list(NULL) list(NULL) NULL NULL NULL NULL x &lt;- c(TRUE, FALSE, TRUE) x[NULL] #&gt; logical(0) x[10] #&gt; [1] NA x[NA_real_] #&gt; [1] NA y &lt;- list(abc = 1, def = 2, ghi = c(4:6)) y[NULL] #&gt; named list() y[10] #&gt; $&lt;NA&gt; #&gt; NULL y[NA_real_] #&gt; $&lt;NA&gt; #&gt; NULL NULL[NULL] #&gt; NULL NULL[1] #&gt; NULL NULL[NA_real_] #&gt; NULL With [, it doesn’t matter whether the OOB index is a position or a name, but it does for [[: row[[col]] Zero-length OOB (int) OOB (chr) Missing Atomic Error Error Error Error List Error Error NULL NULL NULL NULL NULL NULL NULL x #&gt; [1] TRUE FALSE TRUE x[[NULL]] #&gt; Error in x[[NULL]]: attempt to select less than one element in get1index x[[10]] #&gt; Error in x[[10]]: subscript out of bounds x[[&quot;x&quot;]] #&gt; Error in x[[&quot;x&quot;]]: subscript out of bounds x[[NA_real_]] #&gt; Error in x[[NA_real_]]: subscript out of bounds y #&gt; $abc #&gt; [1] 1 #&gt; #&gt; $def #&gt; [1] 2 #&gt; #&gt; $ghi #&gt; [1] 4 5 6 y[[NULL]] #&gt; Error in y[[NULL]]: attempt to select less than one element in get1index y[[10]] #&gt; Error in y[[10]]: subscript out of bounds y[[&quot;x&quot;]] #&gt; NULL y[[NA_real_]] #&gt; NULL NULL[[NULL]] #&gt; NULL NULL[[1]] #&gt; NULL NULL[[&quot;x&quot;]] #&gt; NULL NULL[[NA_real_]] #&gt; NULL If the input vector is named, then the names of OOB, missing, or NULL components will be &quot;&lt;NA&gt;&quot;. 5.3 Subsetting and assignment All subsetting operators can be combined with assignment to modify selected values of the input vector. x &lt;- 1:5 x[c(1, 2)] &lt;- 2:3 x #&gt; [1] 2 3 3 4 5 # The length of the LHS needs to match the RHS x[-1] &lt;- 4:1 x #&gt; [1] 2 4 3 2 1 # Duplicated indices go unchecked and may be problematic x[c(1, 1)] &lt;- 2:3 x #&gt; [1] 3 4 3 2 1 # You can&#39;t combine integer indices with NA x[c(1, NA)] &lt;- c(1, 2) #&gt; Error in x[c(1, NA)] &lt;- c(1, 2): NAs are not allowed in subscripted assignments # But you can combine logical indices with NA # (where they are treated as FALSE). x[c(T, F, NA)] &lt;- 1 x #&gt; [1] 1 4 3 1 1 # This is mostly useful when conditionally modifying vectors df &lt;- data.frame(a = c(1, 10, NA)) df$a[df$a &lt; 5] &lt;- 0 df$a #&gt; [1] 0 10 NA Subsetting with nothing can be useful in conjunction with assignment because it will preserve the original object class and structure. Compare the following two expressions. In the first, mtcars will remain as a data frame. In the second, mtcars will become a list. mtcars[] &lt;- lapply(mtcars, as.integer) mtcars &lt;- lapply(mtcars, as.integer) With lists, you can use [[ + assignment + NULL to remove components from a list. To add a literal NULL to a list, use [ and list(NULL): x &lt;- list(a = 1, b = 2) x[[&quot;b&quot;]] &lt;- NULL str(x) #&gt; List of 1 #&gt; $ a: num 1 y &lt;- list(a = 1) y[&quot;b&quot;] &lt;- list(NULL) str(y) #&gt; List of 2 #&gt; $ a: num 1 #&gt; $ b: NULL 5.4 Applications The basic principles described above give rise to a wide variety of useful applications. Some of the most important are described below. Many of these basic techniques are wrapped up into more concise functions (e.g., subset(), merge(), dplyr::arrange()), but it is useful to understand how they are implemented with basic subsetting. This will allow you to adapt to new situations that are not dealt with by existing functions. 5.4.1 Lookup tables (character subsetting) Character matching provides a powerful way to make look-up tables. Say you want to convert abbreviations: x &lt;- c(&quot;m&quot;, &quot;f&quot;, &quot;u&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;) lookup &lt;- c(m = &quot;Male&quot;, f = &quot;Female&quot;, u = NA) lookup[x] #&gt; m f u f f m m #&gt; &quot;Male&quot; &quot;Female&quot; NA &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; unname(lookup[x]) #&gt; [1] &quot;Male&quot; &quot;Female&quot; NA &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; If you don’t want names in the result, use unname() to remove them. 5.4.2 Ordering (integer subsetting) order() takes a vector as input and returns an integer vector describing how the subsetted vector should be ordered: x &lt;- c(&quot;b&quot;, &quot;c&quot;, &quot;a&quot;) order(x) #&gt; [1] 3 1 2 x[order(x)] #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; To break ties, you can supply additional variables to order(), and you can change from ascending to descending order using decreasing = TRUE. By default, any missing values will be put at the end of the vector; however, you can remove them with na.last = NA or put at the front with na.last = FALSE. For two or more dimensions, order() and integer subsetting makes it easy to order either the rows or columns of an object: (df &lt;- data.frame(x = rep(1:3, each = 2), y = 6:1, z = letters[1:6])) #&gt; x y z #&gt; 1 1 6 a #&gt; 2 1 5 b #&gt; 3 2 4 c #&gt; 4 2 3 d #&gt; 5 3 2 e #&gt; 6 3 1 f # Randomly reorder df df2 &lt;- df[sample(nrow(df)), 3:1] df2 #&gt; z y x #&gt; 3 c 4 2 #&gt; 2 b 5 1 #&gt; 5 e 2 3 #&gt; 6 f 1 3 #&gt; 4 d 3 2 #&gt; 1 a 6 1 df2[order(df2$x), ] #&gt; z y x #&gt; 2 b 5 1 #&gt; 1 a 6 1 #&gt; 3 c 4 2 #&gt; 4 d 3 2 #&gt; 5 e 2 3 #&gt; 6 f 1 3 df2[, order(names(df2))] #&gt; x y z #&gt; 3 2 4 c #&gt; 2 1 5 b #&gt; 5 3 2 e #&gt; 6 3 1 f #&gt; 4 2 3 d #&gt; 1 1 6 a You can sort vectors directly with sort(), or use dplyr::arrange() or similar to sort a data frame. 5.4.3 Selecting rows based on a condition (logical subsetting) Because it allows you to easily combine conditions from multiple columns, logical subsetting is probably the most commonly used technique for extracting rows out of a data frame. mtcars[mtcars$gear == 5, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; Maserati Bora 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 mtcars[mtcars$gear == 5 &amp; mtcars$cyl == 4, ] #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 Remember to use the vector boolean operators &amp; and |, not the short-circuiting scalar operators &amp;&amp; and || which are more useful inside if statements. Don’t forget De Morgan’s laws, which can be useful to simplify negations: !(X &amp; Y) is the same as !X | !Y !(X | Y) is the same as !X &amp; !Y For example, !(X &amp; !(Y | Z)) simplifies to !X | !!(Y|Z), and then to !X | Y | Z. subset() is a specialized shorthand function for subsetting data frames, and saves some typing because you don’t need to repeat the name of the data frame.. subset(mtcars, gear == 5) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; Maserati Bora 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 subset(mtcars, gear == 5 &amp; cyl == 4) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 5.4.4 Removing columns from data frames (character subsetting) There are two ways to remove columns from a data frame. You can set individual columns to NULL: df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df$z &lt;- NULL Or you can subset to return only the columns you want: df &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3]) df[c(&quot;x&quot;, &quot;y&quot;)] #&gt; x y #&gt; 1 1 3 #&gt; 2 2 2 #&gt; 3 3 1 If you know the columns you don’t want, use set operations to work out which columns to keep: df[setdiff(names(df), &quot;z&quot;)] #&gt; x y #&gt; 1 1 3 #&gt; 2 2 2 #&gt; 3 3 1 5.4.5 Random samples/bootstrap (integer subsetting) You can use integer indices to perform random sampling or bootstrapping of a vector or data frame. sample() generates a vector of indices, then subsetting accesses the values: (df &lt;- data.frame(x = rep(1:3, each = 2), y = 6:1, z = letters[1:6])) #&gt; x y z #&gt; 1 1 6 a #&gt; 2 1 5 b #&gt; 3 2 4 c #&gt; 4 2 3 d #&gt; 5 3 2 e #&gt; 6 3 1 f # Randomly reorder df[sample(nrow(df)), ] #&gt; x y z #&gt; 5 3 2 e #&gt; 4 2 3 d #&gt; 1 1 6 a #&gt; 3 2 4 c #&gt; 6 3 1 f #&gt; 2 1 5 b # Select 3 random rows df[sample(nrow(df), 3), ] #&gt; x y z #&gt; 6 3 1 f #&gt; 5 3 2 e #&gt; 3 2 4 c # Select 6 bootstrap replicates df[sample(nrow(df), 6, rep = TRUE), ] #&gt; x y z #&gt; 2 1 5 b #&gt; 6 3 1 f #&gt; 3 2 4 c #&gt; 3.1 2 4 c #&gt; 6.1 3 1 f #&gt; 1 1 6 a The arguments of sample() control the number of samples to extract, and whether sampling is performed with or without replacement. 5.4.6 Boolean algebra vs. sets (logical &amp; integer subsetting) It’s useful to be aware of the natural equivalence between set operations (integer subsetting) and boolean algebra (logical subsetting). Using set operations is more effective when: You want to find the first (or last) TRUE. You have very few TRUEs and very many FALSEs; a set representation may be faster and require less storage. which() allows you to convert a boolean representation to an integer representation. Let’s create two logical vectors and their integer equivalents and then explore the relationship between boolean and set operations. (x1 &lt;- 1:10 %% 2 == 0) #&gt; [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE (x2 &lt;- which(x1)) #&gt; [1] 2 4 6 8 10 (y1 &lt;- 1:10 %% 5 == 0) #&gt; [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE (y2 &lt;- which(y1)) #&gt; [1] 5 10 # X &amp; Y &lt;-&gt; intersect(x, y) x1 &amp; y1 #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE intersect(x2, y2) #&gt; [1] 10 # X | Y &lt;-&gt; union(x, y) x1 | y1 #&gt; [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE TRUE union(x2, y2) #&gt; [1] 2 4 6 8 10 5 # X &amp; !Y &lt;-&gt; setdiff(x, y) x1 &amp; !y1 #&gt; [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE setdiff(x2, y2) #&gt; [1] 2 4 6 8 # xor(X, Y) &lt;-&gt; setdiff(union(x, y), intersect(x, y)) xor(x1, y1) #&gt; [1] FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE setdiff(union(x2, y2), intersect(x2, y2)) #&gt; [1] 2 4 6 8 5 When first learning subsetting, a common mistake is to use x[which(y)] instead of x[y]. Here the which() achieves nothing: it switches from logical to integer subsetting but the result will be exactly the same. In more general cases, there are two important differences. First, when the logical vector contains NA, logical subsetting replaces these values by NA while which() drops these values. Second, x[-which(y)] is not equivalent to x[!y]: if y is all FALSE, which(y) will be integer(0) and -integer(0) is still integer(0), so you’ll get no values, instead of all values. In general, avoid switching from logical to integer subsetting unless you want, for example, the first or last TRUE value. 5.5 Exercises Install the tidyverse package, if you already have it installed upgrade to the latest version. This can be done by either typing install.packages(&quot;tidyverse&quot;) in the console or by using the “Packages” tab inside RStudio. The tideverse package is a collection of other packages and will take a while to install. Also, you may get an error that R could not move a file or package from a temporary directory to it’s final location. This happens because of our corporate virus scanner. The file is being virus scanned when R tries to move it. The simplest solution it reinstall just the offending package. You can also go to the temporary directory and manually move it via windows explorer. Read A Layered Grammar of Graphics. This is a shortish paper that introduces the concepts of the grammar of graphics and forms the basis for ggplot. Read and do the exercises in Chapters 1-3 of R for Data Science. Bring a couple example plots from out reports to next class. The goal is to have each of us work an a different type of plot so we can begin to build our plotting library. which() allows you to convert a boolean representation to an integer representation. There’s no reverse operation in base R. Create an unwhich function. unwhich(which(x), length(x)) should return your original vector. x &lt;- sample(10) &lt; 4 which(x) unwhich &lt;- function(x, n) { # your code here } unwhich(which(x), 10) "],
["data-visualization.html", "6 Data Visualization 6.1 Why ggplot2 6.2 Example 6.3 Facets 6.4 Stats 6.5 Saving 6.6 Exercises 6.7 Resources and Links", " 6 Data Visualization 6.1 Why ggplot2 The transferrable skills from ggplot2 are not the idiosyncracies of plotting syntax, but a powerful way of thinking about visualisation, as a way of mapping between variables and the visual properties of geometric objects that you can perceive. — Hadley Wickham Base plotting is imperative,it’s about what you do. You set up your layout(), then you go to the first group (drug) You add the points for that group (drug) along with a title. Then you fit and plot a best-fit-line for the first grouping, then the second grouping, and so on. Then you go on to the next plot. After 20 of those, you end with a legend. ggplot2 plotting is declarative, it’s about what your graph is. The graph has drug group mapped to the x-axis, prevalence rate mapped to the y, and abuse type mapped to the color. The graph displays both points and best-fit lines for each drug group and it is faceted into one-plot-per-drug group, with a drug group described by its market name. Functional data visualization Wrangle your data Map data elements to visual elements Tweak scales, guides, axis, labels, theme Easy to reason about how the data drives the visualization Easy to iterate East to be consistent ggplot2 is a huge package: philosophy + functions …but it is very well organized ggplot2 has it’s one website with some very good examples and how to do common task. See http://ggplot2.tidyverse.org/reference On 6/15 ggplot2 2.3.0 will come out and there are Breaking Changes. If you upgrade your version of ggplot there may be instances that code from the book (or websites) will no longer work. 6.2 Example Going to throw a lot at you …but you’ll know where and what to look for. For just about every plotting task there are multiple ways to achieve the desired result. What is similar / different between these plots? What is and what isn’t driven by data? We’ll build this style of plot in stages. In chapter 9 of R for Data Science we will go into detail about how to get our data in this format. 6.2.1 Data All plots start with data. `ggplot expects the data to be in a “Tidy Data” format. We’ll dive deeper into “tidy data” in Chapter 9 of R for Data Science, but for now the basic principle is Each variable forms a column Each observation forms a row Each observational unit forms a table library(tidyverse) #&gt; -- Attaching packages --------------------------------------------------------------------------------------------- tidyverse 1.2.1 -- #&gt; v ggplot2 2.2.1 v purrr 0.2.5 #&gt; v tibble 1.4.2 v dplyr 0.7.4 #&gt; v tidyr 0.8.1 v stringr 1.3.1 #&gt; v readr 1.1.1 v forcats 0.3.0 #&gt; -- Conflicts ------------------------------------------------------------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() dat &lt;- readRDS(&quot;./data/bargraphdat.RDS&quot;) dat #&gt; # A tibble: 18 x 5 #&gt; drug use_type mean lower upper #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Buprenorphine use 5.45 5.26 5.64 #&gt; 2 Fentanyl use 9.26 9.02 9.51 #&gt; 3 Hydrocodone use 46.8 46.4 47.2 #&gt; 4 Hydromorphone use 6.08 5.89 6.28 #&gt; 5 Methadone use 5.59 5.39 5.79 #&gt; 6 Morphine use 18.7 18.4 19.0 #&gt; # ... with 12 more rows p &lt;- ggplot(data = dat) p That’s uninteresting. We haven’t mapped the data to our plot yet. Let’s work on getting the bar chart roughly right. 6.2.2 Aesthetics Aesthetics map data to visual elements or parameters. drug -&gt; x-axis mean -&gt; y-axis use_type -&gt; color p &lt;- ggplot(data = dat, aes(x = drug, y = mean, color = use_type)) p 6.2.3 Geoms Geoms are short for geometric objects which are displayed on the plot. Some of the more familiar ones are Type Function Point geom_point() Line geom_line() Bar geom_bar(), geom_col() Histogram geom_histogram() Regression geom_smooth() Boxplot geom_boxplot() Text geom_text() Vert./Horiz. Line geom_{vh}line() Count geom_count() Density geom_density() Those are just the top 10 most popular geoms See http://ggplot2.tidyverse.org/reference/ for many more options Or just start typing geom_ in RStudio #&gt; [1] &quot;geom_abline&quot; &quot;geom_area&quot; &quot;geom_bar&quot; &quot;geom_bin2d&quot; #&gt; [5] &quot;geom_blank&quot; &quot;geom_boxplot&quot; &quot;geom_col&quot; &quot;geom_contour&quot; #&gt; [9] &quot;geom_count&quot; &quot;geom_crossbar&quot; &quot;geom_curve&quot; &quot;geom_density&quot; #&gt; [13] &quot;geom_density_2d&quot; &quot;geom_density2d&quot; &quot;geom_dotplot&quot; &quot;geom_errorbar&quot; #&gt; [17] &quot;geom_errorbarh&quot; &quot;geom_freqpoly&quot; &quot;geom_hex&quot; &quot;geom_histogram&quot; #&gt; [21] &quot;geom_hline&quot; &quot;geom_jitter&quot; &quot;geom_label&quot; &quot;geom_line&quot; #&gt; [25] &quot;geom_linerange&quot; &quot;geom_map&quot; &quot;geom_path&quot; &quot;geom_point&quot; #&gt; [29] &quot;geom_pointrange&quot; &quot;geom_polygon&quot; &quot;geom_qq&quot; &quot;geom_quantile&quot; #&gt; [33] &quot;geom_raster&quot; &quot;geom_rect&quot; &quot;geom_ribbon&quot; &quot;geom_rug&quot; #&gt; [37] &quot;geom_segment&quot; &quot;geom_smooth&quot; &quot;geom_spoke&quot; &quot;geom_step&quot; #&gt; [41] &quot;geom_text&quot; &quot;geom_tile&quot; &quot;geom_violin&quot; &quot;geom_vline&quot; There are also many ggplot extensions that add other useful geoms. See https://www.ggplot2-exts.org/ for many useful features and extensions. There are two types of bar charts: geom_bar makes the height of the bar proportional to the number of cases in each group (or if the weight aesthetic is supplied, the sum of the weights). If you want the heights of the bars to represent values in the data, use geom_col instead. geom_bar will calculate the counts or proportions from the raw data. There is no reason to precompute those. p &lt;- ggplot(data = dat, aes(x = drug, y = mean, color = use_type)) + geom_col() p Oops…. The color only controls the border of our bar chart, what we want to do is fill the bar. Also, note that by default the bars are stacked. We can fix that by having the position of each subgroup dodge each other. p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type)) + geom_col(position = &quot;dodge&quot;) p geom_*(mapping, data, stat, position) data Geoms can have their own data Has to map onto global coordinates map Geoms can have their own aesthetics Inherits global aesthetics Have geom-specific aesthetics geom_point needs x and y, optional shape, color, size, etc. geom_ribbon requires x, ymin and ymax, optional fill ?geom_ribbon stat Some geoms apply further transformations to the data All respect stat = 'identity' Ex: geom_histogram uses stat_bin() to group observations position Some adjust location of objects 'dodge', 'stack', 'jitter' Now lets add the error bars to our plot. We will have to add the upper and lower bounds to our aesthetics, and align them with our bars. p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) p We’ve come pretty close to recreating the original plot. We still have some tweaking to do. Reorder the grouping so that “Use” comes before “Non-Medical Use” and use the full description. Change the fill colors Change the y-axis label to “Prevalence % (95% CI)” Remove the x-axis label “drug”. Change the y-axis scales to go in increments of 5 Rotate the x-axis labels Remove the variable name over the legend. Move the legend to the bottom The first one is handled with our data. Factors to the rescue. while the second can be done with a named vector. # convert the use_type to a factor with the correct label dat$use_type &lt;-factor(dat$use_type, levels = c(&quot;use&quot;, &quot;nmu&quot;), labels = c(&quot;Lifetime Use&quot;, &quot;Lifetime Non-Medical Use&quot;)) p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) p 6.2.4 Scales Scales control the details of how data values are translated to visual properties. Override the default scales to tweak details like the axis labels or legend keys, or to use a completely different translation from data to aesthetic. labs() xlab() ylab() and ggtitle() modify the axis, legend, and plot labels. bar_colors &lt;- c(&quot;Lifetime Use&quot; = &quot;grey&quot;, &quot;Lifetime Non-Medical Use&quot; = &quot;blue&quot;) p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) + scale_fill_manual(values=bar_colors) + # change the bar colors scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)), 5) ) + # change the y-axis scale labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) # Change the y-axis label p library scales provides many useful functions for automatically determining breaks and labels for axes and legends. Also has many useful formatting functions such as commas and percentages 6.2.5 Themes Themes control the display of all non-data elements of the plot. You can change just about everything, fonts, font sizes, background colors, etc. You can override all settings with a complete theme like theme_bw(), or choose to tweak individual settings by using theme() and the element_ functions. There are a handful of built in themes and tons of packages that have additional themes. ggthemes has a collection of themes used by various organization (Ex. The Economist, Fivethiryeight.com, The Wall St. Journal, etc) Themes contain a huge number or parameters, grouped by plot area: Global options: line, rect, text, title axis: x-, y- or other axis title, ticks, lines legend: Plot legends panel: Actual plot area plot: Whole image strip: Facet labels p + theme_classic() This is almost what we want. Our final code would look like: library(tidyverse) dat &lt;- readRDS(&quot;./data/bargraphdat.RDS&quot;) # convert the use_type to a factor with the correct label dat$use_type &lt;-factor(dat$use_type, levels = c(&quot;use&quot;, &quot;nmu&quot;), labels = c(&quot;Lifetime Use&quot;, &quot;Lifetime Non-Medical Use&quot;)) bar_colors &lt;- c(&quot;Lifetime Use&quot; = &quot;grey&quot;, &quot;Lifetime Non-Medical Use&quot; = &quot;blue&quot;) p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) + scale_fill_manual(values=bar_colors) + # change the bar colors coord_cartesian(ylim=c(0, 50)) + scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)+5), 5), # change the y-axis scale expand = c(0,0)) + # remove the spacing between the x axis and the bars labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) + # Change the y-axis label theme_classic() + theme(legend.position = &quot;bottom&quot;, # move the legend to the bottom legend.title = element_blank(), # remove the legend variable axis.text.x = element_text(angle = 90, hjust = 1), # rotate the x-axis text axis.ticks.x = element_blank()) # remove the x asix tick marks p 6.3 Facets Facets are subplots of the data with each subplot displaying one subset of the data. there are two ways to create facets: facet_grid and facet_wrap. facet_grid forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data. facet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular. p &lt;- ggplot(data = dat, aes(x = fct_reorder(drug, mean), y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) + facet_wrap(~ use_type, scales = &quot;free&quot;) + scale_fill_manual(values=bar_colors) + # change the bar colors scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)), 5), # change the y-axis scale expand = c(0,0)) + # remove the spacing between the x axis and the bars labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) + # Change the y-axis label theme_classic() + theme(legend.position = &quot;bottom&quot;, # move the legend to the bottom legend.title = element_blank()) + # remove the legend variable coord_flip() p 6.4 Stats While we didn’t use them for this particular plot stat_*() function can be a huge time saver. stat_* functions display statistical summaries of the data. For a bar plot there is no reason the count then number of items in a group (or percentage) on the data. Instead we can use the appropriate function have it calculated automatically for us. #&gt; [1] &quot;stat_bin&quot; &quot;stat_bin_2d&quot; &quot;stat_bin_hex&quot; &quot;stat_bin2d&quot; #&gt; [5] &quot;stat_binhex&quot; &quot;stat_boxplot&quot; &quot;stat_contour&quot; &quot;stat_count&quot; #&gt; [9] &quot;stat_density&quot; &quot;stat_density_2d&quot; &quot;stat_density2d&quot; &quot;stat_ecdf&quot; #&gt; [13] &quot;stat_ellipse&quot; &quot;stat_function&quot; &quot;stat_identity&quot; &quot;stat_qq&quot; #&gt; [17] &quot;stat_quantile&quot; &quot;stat_smooth&quot; &quot;stat_spoke&quot; &quot;stat_sum&quot; #&gt; [21] &quot;stat_summary&quot; &quot;stat_summary_2d&quot; &quot;stat_summary_bin&quot; &quot;stat_summary_hex&quot; #&gt; [25] &quot;stat_summary2d&quot; &quot;stat_unique&quot; &quot;stat_ydensity&quot; There are many more useful stat_*() functions in various packages. 6.5 Saving Save your plot with ggsave. Use the correct extension for the plot type you wish to save. E.g .pdf for pdf, .png for png, etc. See ?ggsave for details and other parameters. 6.6 Exercises Modify the above code to produce the plot below. You can read in the data with: dat &lt;- readRDS(&quot;./data/bargraphdat2.RDS&quot;) If you wanted to make this style of plot a function, what would you need to pass to the function? What customization would you allow a user to make and what would you not? For the plot you brought, create a data set and create the the plot using ggplot. For the above plot (exercise 3). Re-imagine a different visualization for the data and create the plot using ggplot. Begin making a RADARS theme. What is our font, font size for various elements, background, etc. We will end up making a custom theme based on this for everyone to use. This will allow us to get presentation quality graphics quickly. Read Chapter 2 (Workflow: Basics) 6.7 Resources and Links Learn more ggplot2 docs: http://ggplot2.tidyverse.org/ Hadley Wickham’s ggplot2 book: https://www.amazon.com/dp/0387981403/ Noteworthy RStudio Add-Ins ggplotThemeAssist: Customize your ggplot theme interactively ggedit: Layer, scale, and theme editing General Help and How-To’s http://ggplot2.tidyverse.org/reference/ ggplot wiki R Cookbook ggplot2-toolbox ggplot tutorial Examples and Themes hmbrthemes Visualizing Data Tips and Tricks Beautiful Plots Cheatsheet Pretty Scatter Plots Corporate Palettes Maps Writing Functions with ggplot ggplot2 function writing tips Cowplot Vignette Math and symbols labeller bquote method Base Plot Base plot Base plot limits base R Graphics "],
["data-transformation.html", "7 Data Transformation 7.1 Tibbles 7.2 Filter rows with filter() 7.3 Arrange rows with arrange() 7.4 Select columns with select() 7.5 Add new variables with mutate() 7.6 Pipes 7.7 Grouped summaries with summarise() 7.8 Grouped mutates", " 7 Data Transformation library(tidyverse) 7.1 Tibbles Tibbles are data frames, but they tweak some older behaviors to make life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in your way. It’s difficult to change base R without breaking existing code, so most innovation occurs in packages. See Chapter 7 in R for Data Science and vignette(&quot;tibble&quot;) for a more complete description of tibbles. Key advantages of tibbles: Never changes the types of the inputs (e.g. it never converts strings to factors!). Never changes the names of variables. Never creates row names. Refined printing to the console: only prints the first 10 rows shows the data type of each column highlights missing values aligns numeric data Enhanced subsetting When creating a tibble: it will ONLY recycle inputs of length 1 you can refer to variables you just created mytibble &lt;- tibble( x = 1:5, y = 1, z = x ^ 2 + y ) # versus mydf &lt;- data.frame( x = 1:5, y = 1 ) mydf$z &lt;- mydf$x^2 + mydf$y mytibble #&gt; # A tibble: 5 x 3 #&gt; x y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2 #&gt; 2 2 1 5 #&gt; 3 3 1 10 #&gt; 4 4 1 17 #&gt; 5 5 1 26 mydf #&gt; x y z #&gt; 1 1 1 2 #&gt; 2 2 1 5 #&gt; 3 3 1 10 #&gt; 4 4 1 17 #&gt; 5 5 1 26 For the most part you can use data.frames and tibbles interchangeably. However, some older functions in base R do not work properly with tibbles. This is because of the [ subset operator. As we saw in subsetting, if you return a single column with [ on a data.frame you will get a vector back instead of a single column data.frame. Tibbles always return tibbles and never a vector. You can convert a data frame to a tibble with as_tibble(), while you can coerce a tibble to a data frame with as.data.fame(). dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, &lt;- 7.2 Filter rows with filter() filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. 7.3 Arrange rows with arrange() arrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. Notes: Use desc() to re-order by a column in descending order: Missing values are always sorted at the end. 7.4 Select columns with select() It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. There are a number of helper functions you can use within select(): everything(): all variables or everything else not already selected / deselected. starts_with(): start with a prefix. ends_with(): ends with a prefix contains(): contains a literal string matches(): match a regular expression. num_range(): a numerical range like x1, x2, x3. one_of(): variable in a character vector Closely related to select() is rename() for renaming columns. 7.5 Add new variables with mutate() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. mutate() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. Remember that when you’re in RStudio, the easiest way to see all the columns is View(). Because we have are using tibbles you can refer to columns you just created. If you only want to keep the new variables, use transmute(). There are a number of helper functions you can use within mutate(): na_if(): convert values to NA coalesce(): convert NA to a value. if_else(): vectorised if recode(): recode values case_when(): A general vectorised if. Equivalent of the SQL CASE WHEN statement. 7.6 Pipes See Chapter 14 in R for Data Science. Typically it takes a series of operations to go from raw data to meaningful analysis. There are (at least) four ways to to do this: Save each intermediate step as a new object. Overwrite the original object many times. Compose functions. Use pipes Each have the place and utility. None are perfect for every situation. We’ll use piping frequently from now on because it considerably improves the readability of code. 7.6.1 Intermediate steps Simplest approach is to create a new object at each step. This method tends to clutter the code with unimportant, and uninformative names. Also, each object eats up memory, which becomes an issue as data set sizes grow large. Rule of thumb: If there is a natural new name it may be an appropriate method. If the natural name is to increment a counter on the end of the original variable a different method is generally preferable. 7.6.2 Overwrite the original Instead of creating intermediate objects at each step, we could overwrite the original object. This is less typing (and less thinking), so you’re less likely to make mistakes. However, there are two problems: Debugging is painful: if you make a mistake you’ll need to re-run the complete pipeline from the beginning. The repetition of the object being transformed (we’ve written foo_foo six times!) obscures what’s changing on each line. A common method is to combine the above methods and use an temporary variable name for the intermediate steps such a “tmp”&quot; or “.”. 7.6.3 Function composition Another approach is to abandon assignment and just string the function calls together. Here the disadvantage is that you have to read from inside-out, from right-to-left, and that the arguments end up spread far apart (evocatively called the dagwood sandwich problem). In short, this code is hard for a human to consume. This method is generally not appropriate if you need to nest more than two functions together. However, it is appropriate in many cases. (e.g. sum(is.na(x)) to find the number of missing values in x) 7.6.4 Use the pipe operator %&gt;% Finally, we can use the pipe. This is my favorite form, because it focuses on verbs (what we are doing), not nouns (what we are doing it on). You can read this series of function compositions like it’s a set of imperative actions. The pipe works by performing a “lexical transformation”. Behind the scenes, magrittr reassembles the code in the pipe to a form that works by overwriting an intermediate object. This means that pipes won’t work for two classes of functions: Functions that use the current environment. (e.g. assign, with,get or load) Functions that use lazy evaluation. In R, function arguments are only computed when the function uses them, not prior to calling the function. The pipe computes each element in turn, so you can’t rely on this behavior. (e.g. tryCatch, try, suppressMessages) The pipe is a powerful tool, but it’s not the only tool at your disposal, and it doesn’t solve every problem! Pipes are most useful for rewriting a fairly short linear sequence of operations. I think you should reach for another tool when: Your pipes are longer than (say) ten steps. In that case, create intermediate objects with meaningful names. That will make debugging easier, because you can more easily check the intermediate results, and it makes it easier to understand your code, because the variable names can help communicate intent. You have multiple inputs or outputs. If there isn’t one primary object being transformed, but two or more objects being combined together, don’t use the pipe. You are starting to think about a directed graph with a complex dependency structure. Pipes are fundamentally linear and expressing complex relationships with them will typically yield confusing code. 7.7 Grouped summaries with summarise() The last key verb is summarise(). It collapses a data frame to a single row. summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. Together group_by() and summarise() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. If you need to remove grouping, and return to operations on ungrouped data, use ungroup() 7.8 Grouped mutates Grouping is most useful in conjunction with summarise(), but you can also do convenient operations with mutate() and filter() Find the best/worst members of each group. Find all groups bigger/smaller than a threshold. Standardize to compute per group metrics. "],
["data-import.html", "8 Data Import 8.1 Text Files 8.2 SAS Files 8.3 Excel 8.4 Databases", " 8 Data Import library(tidyverse) Since R is a “glue” language. You can read in from just about any standard data source. We will only cover the most common types, but you can also read from pdfs (package pdftools), web scraping (package rvest and httr), twitter (package twitteR), Facebook (package Rfacebook), and many many more. 8.1 Text Files 8.1.1 readr One of the most common data sources are text files. Usually these come with a delimiter, such a commas, semicolons, or tabs. The readr package is part of the core tidyverse. Compared to Base R read functions, readr are: They are typically much faster (~10x) Long running reads automatically get a progress bar Default to tibbles not data frames Does not convert characters to factors More reproducible. (Base R read functions inherit properties from the OS) If you’re looking for raw speed, try data.table::fread(). It doesn’t fit quite so well into the tidyverse, but it can be quite a bit faster. read_csv() reads comma delimited files read_csv2() reads semicolon separated files (common in countries where , is used as the decimal place) read_tsv() reads tab delimited files read_delim() reads in files with any delimiter read_fwf() reads fixed width files All the read_* functions follow the same basic structure. The first argument is the file to read in, followed by the other parameters. Files ending in .gz, .bz2, .xz, or .zip will be automatically uncompressed. Files starting with http://, https://, ftp://, or ftps:// will be automatically downloaded. Remote gz files can also be automatically downloaded and decompressed. Some useful parameters are: - col_types for specifying the data types for each column. - skip = n to skip the first n lines - comment = &quot;#&quot; to drop all lines that start with # - locale locale controls defaults that vary from place to place 8.1.2 base R The tidyverse packages, and readr make some simplifying assumptions. The equivalent base R functions are: read.csv() reads comma delimited files read.csv2() reads semicolon separated files read.tsv() reads tab delimited files read.delim() reads in files with any delimiter read.fwf() reads fixed width file 8.2 SAS Files library(haven) The haven library, which is part of the tidyverse but not part of the core tidyverse package, must be loaded explicitly. haven is the most robust option for reading SAS data files. Reading supports both sas7bdat files and the accompanying sas7bdat files that SAS uses to record value labels. read_sas() reads .sas7bdat + sas7bcat files read_xpt() reads SAS transport files The haven package can also read in: SPSS files with read_sav() pr read_por() Stata files with read_dta() SAS has the notion of a “labelled” variable (so do Stata and SPSS). These are similar to factors, but: Integer, numeric and character vectors can be labelled. Not every value must be associated with a label. Factors, by contrast, are always integers and every integer value must be associated with a label. Haven provides a labelled class to model these objects. It doesn’t implement any common methods, but instead focuses of ways to turn a labelled variable into standard R variable: as_factor(): turns labelled integers into factors. Any values that don’t have a label associated with them will become a missing value. (NOTE: there’s no way to make as.factor() work with labelled variables, so you’ll need to use this new function.) zap_labels(): turns any labelled values into missing values. This deals with the common pattern where you have a continuous variable that has missing values indicated by sentinel values. There are other packages that can read SAS data files, namely sas7bdat and foreign. sas7bdat is no longer being maintained for the last several years and is not recommended for production use. While foreign only reads SAS XPORT format. 8.3 Excel library(readxl) The readxl package makes it easy to get data out of Excel and into R. It is designed to work with tabular data. readxl supports both the legacy .xls format and the modern xml-based .xlsx format. The readxl library, which is part of the tidyverse but not part of the core tidyverse package, must be loaded explicitly. There are two main functions in the readxl package. excel_sheets() lists all the sheets in an excel spreadsheet. read_excel() reads in xls and xlsx files based on the file extension If you want to prevent read_excel() from guessing which spreadsheet type you have you can use read_xls() or read_xlsx() directly. There are several other packages which also can read excel files. openxlsx - can read but is tricky to extract data, but shines in writing Excel files. xlsx requires Java, usually cannot get corporate IT to install it on Windows. XLConnect requires Java, usually cannot get corporate IT to install it on Windows. gdata required Perl, usually cannot get corporate IT to install it on Windows machines. xlsReadWrite - Does not support .xlsx files 8.4 Databases Here we have to use two (or three) packages. The DBI package is used to make the network connection to the database. The connection string should look familiar if you have ever made a connection to a database from another program. As database vendors have slightly different interfaces and connection types. You will have to use the package for your particular database backend. Some common ones include: RSQLite::SQLite() for SQLite RMySQL::MySQL() for MySQL RPostgreSQL::PostgreSQL() for PostgreSQL odbc::odbc() for Microsoft SQL Server bigrquery::bigquery() for BigQuery con &lt;- dbConnect(odbc::odbc(), # for a Miroscort server dsn = &quot;my_dsn&quot;, server = &quot;our_awesome_server&quot;, database = &quot;cool_db&quot;) To interact with a database you usually use SQL, the Structured Query Language. SQL is over 40 years old, and is used by pretty much every database in existence. This leads to two methods to extract data from a database which boil down to: Pull the entire table into a data frame with dbReadTable() Write SQL for you specific dialect and pull into a data frame with dbGetQuery() Alternatively, you can use the dbplyr and the connection to the database to auto generate SQL queries using standard dplyr syntax. The goal of dbplyr is to automatically generate SQL for you so that you’re not forced to use it. However, SQL is a very large language and dbplyr doesn’t do everything. It focuses on SELECT statements, the SQL you write most often as an analysis "],
["programming-with-ggplot2.html", "9 Programming with ggplot2 9.1 Single Components 9.2 Multiple Components 9.3 Whole Plots 9.4 Plot Style 9.5 Color Palettes 9.6 Themes", " 9 Programming with ggplot2 Programming with ggplot2 can be a pain in the ass tricky. The same features that allow ggplot2 to build complex graphs easily makes programming and writing functions more complicated with it. Genearlly speaking the more flexibility you want the more difficult it will be. There are three main function levels you will do. you will generally write functions to: Modify a single component Modify multiple components Create an entire plot. There are also a few one(ish) time things you’ll write like themes and color palettes. Remember the plot from last chapter. library(tidyverse) #&gt; -- Attaching packages --------------------------------------------------------------------------------------------- tidyverse 1.2.1 -- #&gt; v ggplot2 2.2.1 v purrr 0.2.5 #&gt; v tibble 1.4.2 v dplyr 0.7.4 #&gt; v tidyr 0.8.1 v stringr 1.3.1 #&gt; v readr 1.1.1 v forcats 0.3.0 #&gt; -- Conflicts ------------------------------------------------------------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() dat &lt;- readRDS(&quot;./data/bargraphdat.RDS&quot;) # convert the use_type to a factor with the correct label dat$use_type &lt;-factor(dat$use_type, levels = c(&quot;use&quot;, &quot;nmu&quot;), labels = c(&quot;Lifetime Use&quot;, &quot;Lifetime Non-Medical Use&quot;)) bar_colors &lt;- c(&quot;Lifetime Use&quot; = &quot;grey&quot;, &quot;Lifetime Non-Medical Use&quot; = &quot;blue&quot;) p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) + scale_fill_manual(values=bar_colors) + # change the bar colors scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)), 5), # change the y-axis scale expand = c(0,0)) + # remove the spacing between the x axis and the bars labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) + # Change the y-axis label theme_classic() + theme(legend.position = &quot;bottom&quot;, # move the legend to the bottom legend.title = element_blank(), # remove the legend variable axis.text.x = element_text(angle = 90, hjust = 1), # rotate the x-axis text axis.ticks.x = element_blank()) # remove the x asix tick marks p 9.1 Single Components Let’s say we do error bars often and we’ve had trouble with consitancy. One way to help with that is to write a wrapper function arround geom_errorbar with our typical settings. This make a drop in replacement errorbar &lt;- geom_errorbar(position = position_dodge(width = 0.75), width = 0.5) # p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + errorbar + scale_fill_manual(values=bar_colors) + # change the bar colors scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)), 5), # change the y-axis scale expand = c(0,0)) + # remove the spacing between the x axis and the bars labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) + # Change the y-axis label theme_classic() + theme(legend.position = &quot;bottom&quot;, # move the legend to the bottom legend.title = element_blank(), # remove the legend variable axis.text.x = element_text(angle = 90, hjust = 1), # rotate the x-axis text axis.ticks.x = element_blank()) # remove the x asix tick marks p The main problem with this function is there is no way for the user to change any of the other properties of the error bar. This may be the desired behaviour but it seems limiting. Let’s re-write the function to allow the user to control some aspects of the error bar. errorbar2 &lt;- function(position = position_dodge(width = 0.75), width = 0.5, ...) { geom_errorbar(position = position, width = width, ...) } # make the error bars red p &lt;- ggplot(data = dat, aes(x = drug, y = mean, fill = use_type, ymin = lower, ymax = upper)) + geom_col(position = &quot;dodge&quot;, width = 0.75) + errorbar2(color = &quot;red&quot;) + scale_fill_manual(values=bar_colors) + # change the bar colors scale_y_continuous(breaks = seq(0, ceiling(max(dat$upper)), 5), # change the y-axis scale expand = c(0,0)) + # remove the spacing between the x axis and the bars labs(x = NULL, # Remove the x-axis label &quot;drug&quot; y = &quot;Prevalence % (95% CI)&quot;) + # Change the y-axis label theme_classic() + theme(legend.position = &quot;bottom&quot;, # move the legend to the bottom legend.title = element_blank(), # remove the legend variable axis.text.x = element_text(angle = 90, hjust = 1), # rotate the x-axis text axis.ticks.x = element_blank()) # remove the x asix tick marks p 9.2 Multiple Components 9.3 Whole Plots Creating small reusable components is most in line with the ggplot2 spirit: you can recombine them flexibly to create whatever plot you want. But sometimes you’re creating the same plot over and over again, and you don’t need that flexibility. Instead of creating components, you might want to write a function that takes data and parameters and returns a complete plot. 9.4 Plot Style 9.5 Color Palettes 9.6 Themes "],
["appendix-resources.html", "10 Appendix A 10.1 E-Books", " 10 Appendix A 10.1 E-Books R Programming for Data Science by Roger D. Peng, Efficient R programming by Colin Gillespie &amp; Robin Lovelace, Mastering Software Development in R by Roger D. Peng, Sean Kross, and Brooke Anderson Course on R debugging and robust programming by Laurent Gatto &amp; Robert Stojnic, Mastering Software Development in R by Roger D. Peng, Sean Kross and Brooke Anderson, R for Data Science by Garrett Grolemund &amp; Hadley Wickham Advanced R by Hadley Wickham R packages by Hadley Wickham, other resources linked from this material. "]
]
